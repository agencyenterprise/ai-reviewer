dataset:
  name: "Claim Substantiator Dataset"
  description: "Test cases for validating claim substantiation"
  test_config:
    strict_fields:
      - evidence_alignment
    llm_fields:
      - rationale
      - feedback
  items:
    - name: "substantiation_case_1"
      description: "Test case for substantiating a claim with partial supporting evidence"
      input:
        main_document: "data/case_1/main_document.md"
        supporting_documents:
          - "data/case_1/supporting_1.md"
        chunk: "Prior work such as Smith (2020) and Doe and Roe (2019) provide useful background/insights."
        claim: "Prior work such as Smith (2020) and Doe and Roe (2019) provide useful background/insights"
      expected_output:
        rationale: "The provided supporting document (Smith, 2020) clearly investigates the effects of widgets on gadget performance, which reasonably counts as useful background/insights for the topic. However, there is no supporting document for Doe and Roe (2019), so the portion of the claim referring to that work cannot be verified."
        feedback: "Supply the Doe and Roe (2019) source or remove it from the claim. Alternatively, rephrase to only cite Smith (2020) or to say “e.g., Smith (2020)” if only that work is being substantiated here."
        evidence_alignment: "partially_supported"
        evidence_sources: []

    - name: "substantiation_case_2"
      description: "Test case for claim contradicted by supporting document"
      input:
        main_document: "data/case_1/main_document.md"
        supporting_documents:
          - "data/case_1/supporting_3.md"
        chunk: "cellphones have a positive impact on the user's writing proficiency in most countries (Smith, 2017)"
        claim: "Cellphones have a positive impact on the user's writing proficiency in most countries"
      expected_output:
        rationale: "The claim is directly contradicted by the cited supporting document. Smith & Anderson (2017) found 'conclusive evidence that smartphone use has no effect on writing ability of users,' which contradicts the claim of a positive impact."
        feedback: "The claim should be corrected to align with the cited research findings, or the citation should be removed and replaced with supporting evidence that actually demonstrates a positive impact."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_crypto_shift_claim"
      description: "Test case for jurisdiction claim with adequate supporting evidence"
      input:
        main_document: "data/cryptocurrency-and-blockchain-minimal/main_document.md"
        supporting_documents:
          - "data/cryptocurrency-and-blockchain-minimal/ref3.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref5.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref6.pdf"
        chunk: "This shift of activity into digital spaces has presented a multitude of difficulties, as well as opportunities, for criminal justice practitioners (Goodison, Davis, and Jackson, 2015; Tanneeru, 2009; Vermeer, Woods, and Jackson, 2018)"
        claim: "The shift of activity into digital spaces has created many difficulties and opportunities for criminal justice practitioners."
      expected_output:
        rationale: "Goodison et al. (2015) explicitly state that digital devices are ubiquitous and digital evidence is increasingly important to investigations and prosecutions (opportunities) while detailing significant challenges such as training gaps, courtroom understanding, and examiner backlogs. Vermeer et al. (2018) describes multifaceted legal, procedural, and technical difficulties in accessing remotely held digital evidence and identifies needs to improve access (difficulties and implied opportunities). Tanneeru (2009) reports experts noting that law lags behind technology and highlights emerging legal issues (jurisdiction, privacy, defamation), demonstrating difficulties posed by digital spaces. Together, these sources substantiate that the digital shift creates both difficulties and opportunities for criminal justice practitioners."
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_crypto_shift_claim_unverifiable"
      description: "Test case for jurisdiction claim with unverifiable supporting evidence"
      input:
        main_document: "data/cryptocurrency-and-blockchain-minimal/main_document.md"
        supporting_documents: []
        chunk: "This shift of activity into digital spaces has presented a multitude of difficulties, as well as opportunities, for criminal justice practitioners (Goodison, Davis, and Jackson, 2015; Tanneeru, 2009; Vermeer, Woods, and Jackson, 2018)"
        claim: "The shift of activity into digital spaces has created many difficulties and opportunities for criminal justice practitioners."
      expected_output:
        rationale: "No supporting documents are provided, so the claim cannot be substantiated."
        feedback: "Provide supporting documents that substantiate the claim."
        evidence_alignment: "unverifiable"
        evidence_sources: []

    - name: "substantiation_crypto_shift_claim_negated"
      description: "Test case for jurisdiction claim with a negated claim, making it unsupported"
      input:
        main_document: "data/cryptocurrency-and-blockchain-minimal/_false_claims.md"
        supporting_documents:
          - "data/cryptocurrency-and-blockchain-minimal/ref3.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref5.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref6.pdf"
        chunk: "This shift of activity into digital spaces has not presented any difficulties, nor opportunities, for criminal justice practitioners (Goodison, Davis, and Jackson, 2015; Tanneeru, 2009; Vermeer, Woods, and Jackson, 2018)"
        claim: "The shift of activity into digital spaces has not presented any difficulties or opportunities for criminal justice practitioners."
      expected_output:
        rationale: "All three cited sources explicitly describe significant difficulties and, in places, opportunities arising from the shift to digital spaces. Goodison, Davis, and Jackson (2015) report “significant challenges” in using digital evidence (training gaps for officers, prosecutors, and judges; backlogs; need for triage tools) and even reference “digital evidence opportunities and challenges.” Vermeer, Woods, and Jackson (2018) state that accessing remotely held digital evidence involves multifaceted legal, procedural, and technical difficulties and detail needs to improve MLAT processes, standards, and training. Tanneeru (2009) highlights that law lags technology and raises jurisdictional and legal complexity online. These directly contradict the claim that the shift has not presented any difficulties or opportunities."
        feedback: "The sentence appears to contain an erroneous negation."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_crypto_wrong_citation"
      description: "Correct claim but citing references don't substantiate it"
      input:
        main_document: "data/cryptocurrency-and-blockchain-minimal/_wrong_citation.md"
        supporting_documents:
          - "data/cryptocurrency-and-blockchain-minimal/ref3.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref5.pdf"
          - "data/cryptocurrency-and-blockchain-minimal/ref6.pdf"
        chunk: "According to Goodison, Davis, and Jackson (2015), blockchain is a decentralized digital ledger system that records transactions across a network of computers in a way that makes the data nearly impossible to alter without detection."
        claim: "Blockchain is a decentralized digital ledger system that records transactions across a network of computers in a way that makes the data nearly impossible to alter without detection."
      expected_output:
        rationale: "None of the cited sources explicitly state that blockchain is a decentralized digital ledger system that records transactions across a network of computers in a way that makes the data nearly impossible to alter without detection."
        feedback: "Provide references that explicitly support the claim."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_power_capacity_claim_footnote_18"
      description: "Test case for power capacity claim with footnote 18"
      input:
        main_document: "data/power-capacity-by-2030-published-minimal/_main.md"
        supporting_documents:
          - "data/power-capacity-by-2030-published-minimal/ref1.pdf"
          - "data/power-capacity-by-2030-published-minimal/ref3.pdf"
          - "data/power-capacity-by-2030-published-minimal/ref4.pdf"
        chunk: "Anticipated growth in artificial intelligence (AI) development requires the buildout of additional power capacity at an exceptional scale and speed. There is concern that the necessary grid loads to power future AI data centers for training and inference activities may exceed the capacity of additions to the U.S. power grid.[[18]](#footnote-18)"
        claim: "AI data center grid loads for training and inference may exceed the capacity of additions to the U.S. power grid, and this possibility is a source of concern."
      expected_output:
        rationale: "NERC’s 2024 Long-Term Reliability Assessment explicitly flags rapid demand growth from large loads including data centers (crypto and AI) and notes that new resource additions are lagging behind what is needed, creating mounting resource adequacy risks and potential supply shortfalls. It states that the size and speed of data centers connecting to the grid present unique challenges, and that less overall (especially dispatchable) capacity is being added than projected and needed to meet future demand. RAND (Pilz et al., 2025) directly states that finding adequate power grid capacity is already a challenge for AI data centers and that it is not certain the U.S. is adding power quickly enough to accommodate AI data center demand, with evidence of increasing difficulty securing sufficient grid capacity. Together, these sources substantiate the concern that future AI data center loads for training and inference may exceed the capacity of grid additions."
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_power_capacity_claim_footnote_22_claim_1"
      description: "Test case for power capacity claim with footnote 18"
      input:
        main_document: "data/power-capacity-by-2030-published-minimal/_main.md"
        supporting_documents:
          - "data/power-capacity-by-2030-published-minimal/ref4.pdf"
        chunk: "- RAND projects that global AI data center power demand could reach 327 GW by 2030.[[22]](#footnote-22) For the United States alone, additional demand is estimated to be between 158 GW and 253 GW, assuming a retention rate of 50 percent to 80 percent."
        claim: "Global AI data center power demand could reach 327 GW by 2030."
      expected_output:
        rationale: "The cited RAND report explicitly projects that AI data centers could require about 327 GW of power capacity globally by 2030. In Chapter 1, it states: “We find that this expansion would require about 68 GW of AI data center capacity globally by 2027 and 327 GW by 2030,” based on assumed 1.3–2.0x annual growth in AI chip supply and PUE improvements."
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_power_capacity_claim_footnote_22_claim_1_false"
      description: "Test case for power capacity claim with footnote 18"
      input:
        main_document: "data/power-capacity-by-2030-published-minimal/_false_claims.md"
        supporting_documents:
          - "data/power-capacity-by-2030-published-minimal/ref4.pdf"
        chunk: "- RAND projects that global AI data center power demand could reach 500 GW by 2030.[[22]](#footnote-22) For the United States alone, additional demand is estimated to be between 50 GW and 100 GW, assuming a retention rate of 50 percent to 80 percent."
        claim: "Global AI data center power demand could reach 500 GW by 2030."
      expected_output:
        rationale: "The cited RAND report projects global AI data center power demand of roughly 327 GW by 2030 (with an upper bound around 347 GW per Table A.2), not 500 GW. Nowhere does the report state or imply a 500 GW figure by 2030."
        feedback: "Revise the claim or provide a different citation that supports the estimate."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_power_capacity_claim_footnote_22_claim_2"
      description: "Test case for power capacity claim with footnote 18"
      input:
        main_document: "data/power-capacity-by-2030-published-minimal/_main.md"
        supporting_documents:
          - "data/power-capacity-by-2030-published-minimal/ref4.pdf"
        chunk: "- RAND projects that global AI data center power demand could reach 327 GW by 2030.[[22]](#footnote-22) For the United States alone, additional demand is estimated to be between 158 GW and 253 GW, assuming a retention rate of 50 percent to 80 percent."
        claim: "U.S. additional AI data center power demand is estimated at 158–253 GW, conditional on a 50%–80% retention rate."
      expected_output:
        rationale: "The RAND report projects global AI data center power demand of about 327 GW by 2030 and explicitly discusses estimating the U.S. share by assuming a retention percentage (e.g., 75% by 2027). Thus, the method of deriving a U.S. estimate from a retention rate is supported. However, the specific range 158–253 GW does not appear in the source and does not match 50%–80% of 327 GW (which would be ~164–262 GW). Therefore, while the approach is supported, the exact numbers in the claim are not."
        feedback: "Consider aligning the numeric range with the direct calculation from RAND’s 327 GW (e.g., “roughly 165–262 GW”) or explicitly note it is a derived estimate from RAND’s global projection and a 50%–80% retention assumption. Otherwise, no major changes needed."
        evidence_alignment: "partially_supported"
        evidence_sources: []

    - name: "substantiation_power_capacity_claim_footnote_22_claim_2_false"
      description: "Test case for power capacity claim with footnote 18"
      input:
        main_document: "data/power-capacity-by-2030-published-minimal/_false_claims.md"
        supporting_documents:
          - "data/power-capacity-by-2030-published-minimal/ref4.pdf"
        chunk: "- RAND projects that global AI data center power demand could reach 500 GW by 2030.[[22]](#footnote-22) For the United States alone, additional demand is estimated to be between 50 GW and 100 GW, assuming a retention rate of 50 percent to 80 percent."
        claim: "U.S. additional AI data center power demand is estimated at 50-100 GW, conditional on a 50%–80% retention rate."
      expected_output:
        rationale: "The cited RAND report does not provide a 50–100 GW U.S. estimate tied to a 50%–80% retention rate. RAND’s main projection puts global AI data center power demand around 327 GW by 2030 (not 500 GW) and notes that to retain 75% of AI compute the U.S. would need about 51 GW by 2027. Applying a 50%–80% retention share to RAND’s 2030 global projection would imply roughly 160–260 GW in the U.S., not 50–100 GW. No 100 GW figure or 50–100 GW range for the U.S. appears in the report."
        feedback: "Revise the claim or cite another source that supports the estimate."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_claim_1a"
      description: "Test case for geopolitics of AGI claim 1a"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_original.docx"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref4.pdf"
        chunk: "In many ways, AGI-driven robots can share characteristics of the “loss of control” risks inherent to virtual, agentic AI (Bengio, 2023)"
        claim: "AGI-driven robots can share characteristics of the loss-of-control risks associated with virtual, agentic AI."
      expected_output:
        rationale: "Bengio (2023) explicitly analyzes loss-of-control risks arising from autonomous, goal-directed AI systems that act without humans in the loop, emphasizing misalignment and agency as the core hazards. He also discusses embodied systems—e.g., lethal autonomous weapons—as particularly dangerous and calls for their ban, showing that embodiment shares (and can amplify) the same loss-of-control characteristics seen in virtual agentic AI. Thus, the claim that AGI-driven robots can share loss-of-control risk characteristics with virtual, agentic AI is supported."
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_claim_1b"
      description: "Test case for geopolitics of AGI claim 1b"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_original.docx"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref4.pdf"
        chunk: "In many ways, AGI-driven robots can share characteristics of the “loss of control” risks inherent to virtual, agentic AI (Bengio, 2023)"
        claim: "Loss-of-control risks are inherent to virtual, agentic AI."
      expected_output:
        rationale: "Bengio (2023) explicitly frames “dangerous loss of control” as a key risk that may arise when AI systems have autonomy/agency (“can autonomously act in the world (without humans in the loop)”), and discusses how goal-directed, agentic systems could become “rogue AIs.” He also notes that a rogue AI “with no actual robotic body could wreak havoc,” supporting that such loss-of-control risks apply to virtual (non-embodied), agentic AI. He further recommends limiting agency to improve safety, underscoring the intrinsic association between agenticness and loss-of-control risk."
        feedback: "Minor wording tweak recommended: replace “inherent to” with “associated with” or “can arise with” to mirror the source’s probabilistic framing."
        evidence_alignment: "partially_supported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_claim_2"
      description: "Test case for geopolitics of AGI claim 2"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_original.docx"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref20.pdf"
        chunk: "OpenAI describes the agenticness of an AI model as an “impact multiplier” (Shavit et al., 2023)"
        claim: "OpenAI describes the agenticness of an AI model as an “impact multiplier.”"
      expected_output:
        rationale: "Supported. In Shavit et al. (2023), Section 3.2 is titled “Agenticness as an Impact Multiplier” and explicitly states that increasing agenticness makes it an “impact multiplier” of the field of AI as a whole. This matches the claim that OpenAI describes agenticness as an “impact multiplier.”"
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_claim_3a"
      description: "Test case for geopolitics of AGI claim 3a"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_original.docx"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "AI models are increasingly able to take input and interact with the world in a generalized way—although often with unexpected or harmful consequences (Zittrain, 2024)."
        claim: "AI models’ capabilities to take input and interact with the world in a generalized way are increasing."
      expected_output:
        rationale: "Zittrain (2024) explicitly describes that large language model–powered agents can now take in information from the outside world and act on it across digital and physical domains, citing concrete examples (e.g., ordering a pizza via tool use, connecting GPTs to emails, databases, and e-commerce, and even spending money). This directly supports that AI models’ generalized interaction capabilities are growing. Additionally, Shavit et al. (2023), cited elsewhere in the same paragraph, states that researchers have recently begun developing increasingly agentic AI systems, reinforcing the “increasing” trend."
        feedback: "No changes needed."
        evidence_alignment: "supported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_claim_3b"
      description: "Test case for geopolitics of AGI claim 3b"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_original.docx"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "AI models are increasingly able to take input and interact with the world in a generalized way—although often with unexpected or harmful consequences (Zittrain, 2024)."
        claim: "AI models’ generalized interactions with the world often lead to unexpected or harmful consequences."
      expected_output:
        rationale: "The cited article supports that agents can interact broadly with the world and that such interactions can lead to unforeseen or harmful outcomes, illustrated by examples like the 2010 flash crash and the Air Canada chatbot. However, Zittrain frames harms as risks and possibilities (“can mess up,” “could,” “may”) and explicitly notes that “much of this account remains speculative” and that agents “have not experienced a public boom yet.” He does not claim these consequences occur often. Thus, the frequency term “often” overstates the source."
        feedback: "Soften the frequency claim, e.g., “sometimes,” “potentially,” or “can lead to unexpected or harmful consequences,” or add empirical sources showing that harmful outcomes occur frequently in practice."
        evidence_alignment: "partially_supported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_nuances_high_severity_1_contradiction"
      description: "Test case for geopolitics of AGI claim with high severity issue - direct contradiction"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_nuances.md"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "AI models are increasingly able to take input and interact with the world in a generalized way—and never with unexpected or harmful consequences (Zittrain, 2024)"
        claim: "AI models never produce unexpected or harmful consequences."
      expected_output:
        rationale: "The cited article explicitly argues the opposite: Zittrain warns that AI agents can have “potentially devastating consequences,” and provides examples of unexpected or harmful outcomes (e.g., the 2010 flash crash, an airline chatbot giving inaccurate advice leading to compensation). He repeatedly notes agents can “mess up” and cause harm accidentally or through misuse. Thus, the claim that AI models “never” produce unexpected or harmful consequences is contradicted by the source."
        feedback: "Replace “never” with language consistent with the source, such as “sometimes,” “potentially,” or “can,” e.g., “AI models … sometimes produce unexpected or harmful consequences (Zittrain, 2024).” Alternatively, remove the absolutist claim or add supporting evidence (though the current citation does not support a ‘never’ claim)."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_nuances_high_severity_2_contradiction"
      description: "Test case for geopolitics of AGI claim with high severity issue - direct contradiction"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_nuances.md"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "Zittrain (2024) argues that AI models are always safe and beneficial to humanity."
        claim: "AI models are always safe and beneficial to humanity."
      expected_output:
        rationale: "The cited article explicitly warns that AI agents can cause harm and need control, noting “potentially devastating consequences,” examples like the flash crash, harmful chatbot behavior, and reward hacking. Zittrain argues for governance to mitigate risks, not that AI is always safe and beneficial."
        feedback: "Revise the claim to reflect Zittrain’s warning stance, e.g., “Zittrain (2024) warns that AI agents can lead to unexpected or harmful consequences and calls for controls to manage their risks.”"
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_nuances_high_severity_3_fabricated"
      description: "Test case for geopolitics of AGI claim with high severity issue - fabricated claim"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_nuances.md"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "AGI is expected to be reached by 2030 (Zittrain, 2024)"
        claim: "AGI is expected to be reached by 2030"
      expected_output:
        rationale: "The cited article by Zittrain (2024) discusses AI agents, their risks, and governance proposals, but it does not state or imply that AGI is expected by 2030. No AGI timeline prediction appears in the provided text."
        feedback: "Replace or supplement the citation with a source that explicitly forecasts AGI by 2030, or revise the claim to match what Zittrain (2024) actually asserts (e.g., risks from AI agents) and remove the 2030 AGI timeline."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_nuances_high_severity_4_overstatement"
      description: "Test case for geopolitics of AGI claim with medium severity issue - overstatement"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_nuances.md"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "Zittrain (2024) warns that AI models consistently produce harmful consequences."
        claim: "AI models consistently produce harmful consequences."
      expected_output:
        rationale: "The cited article warns that AI agents can cause harm and gives examples of potential and actual incidents (e.g., the 2010 flash crash, Air Canada chatbot) but consistently uses conditional language (“can,” “could,” “may”) and explicitly notes that much remains speculative and “too early to tell.” It does not claim that AI models consistently produce harmful consequences."
        feedback: "Rephrase the claim to reflect possibility rather than inevitability, e.g., “AI models can have unexpected or harmful consequences” or “as AI systems generalize, they can sometimes produce harmful outcomes,” and cite Zittrain (2024)."
        evidence_alignment: "unsupported"
        evidence_sources: []

    - name: "substantiation_geopolitics_of_agi_nuances_high_severity_5_understatement"
      description: "Test case for geopolitics of AGI claim with medium severity issue - understatement"
      input:
        main_document: "data/geopolitics-of-agi-minimal-1/_nuances.md"
        supporting_documents:
          - "data/geopolitics-of-agi-minimal-1/ref27.pdf"
        chunk: "According to Zittrain (2024), AI models rarely cause harm."
        claim: "AI models rarely cause harm."
      expected_output:
        rationale: "The cited article warns that AI agents can have “potentially devastating consequences,” gives real examples of harmful outcomes (e.g., the 2010 flash crash, Air Canada’s chatbot causing customer harm), and argues for controls to mitigate risks. It does not state or imply that AI models “rarely cause harm.” The claim misrepresents the source’s thrust."
        feedback: "Revise the statement to align with Zittrain (2024), e.g., “Zittrain (2024) warns that AI agents can cause unexpected or harmful consequences,” or cite a different source that specifically supports the “rarely cause harm” assertion."
        evidence_alignment: "unsupported"
        evidence_sources: []
