[...]

Introduction

Millions of robots are in use around the world today—including autonomous automobiles, drones, and household and business robots. These robots can be thought of as the arms and legs of a cyber-physical body, and their numbers are projected to increase significantly in the 2020s. The brains of that body are the artificial intelligence (AI) tools that give robots their instructions. More than 300 million people used AI tools every month in 2024, and their numbers are expected to double by 2028 (Statista, 2024).

In this paper, we assess the systemic risks that could be created by the combination of robots and artificial general intelligence (AGI). Both the global number of AI tool users and the robots of all types that might embody AI agents dwarf the size of the U.S. federal civilian and military workforce (Figure 1). The consequences of this imbalance could be severe: The people charged with overseeing, regulating, and protecting us from potential harms will be vastly outnumbered by the robotic platforms operating in the United States. If an adversary were able to employ AI to scalably assert control over these platforms, it would create the potential for a catastrophic attack on the country.

Soon, human users might have even more-capable AI tools available to them. Notably, Sam Altman, the CEO of OpenAI, has projected that AGI—a form of AI capable of performing any intellectual task that a human can do—could be achieved before 2030 (Okemwa, 2024). And those AGI tools might be used to create myriad powerful AI agents to assist them. Figure 1 depicts the total number of potential robotic embodiments for AI agents that might exist this decade.

[...]

# Threat Model

[...]

In many ways, AGI-driven robots can share characteristics of the “loss of control” risks inherent to virtual, agentic AI (Bengio, 2023). OpenAI describes the agenticness of an AI model as an “impact multiplier” (Shavit et al., 2023). AI models are increasingly able to take input and interact with the world in a generalized way—although often with unexpected or harmful consequences (Zittrain, 2024). The consequences of loss of control would be further amplified if an AI agent deployed by an adversary state were embodied and able to influence the physical world with its generalized capabilities. The agent could constitute a robotic insider threat in any location where it was used, potentially allowing an attacker to be one software update or hack away from remotely executing a wide variety of actions in the physical world. A sophisticated adversary—e.g., a nation-state or even a rogue AGI agent—could conceivably exploit a vulnerability common to many robots to issue malicious instructions that would be carried out with intelligent autonomy. This could allow an attacker to use proliferated robotic platforms to execute a wide-ranging attack on the United States—both to disrupt and to destroy infrastructure and terrorize citizens.

[...]

# Tests

AI models are increasingly able to take input and interact with the world in a generalized way—although often with unexpected or harmful consequences (Zittrain, 2024).

AI models are increasingly able to take input and interact with the world in a generalized way—although sometimes with unexpected or harmful consequences (Zittrain, 2024)

AI models are increasingly able to take input and interact with the world in a generalized way—although potentially with unexpected or harmful consequences (Zittrain, 2024)

Zittrain (2024) observes that as AI systems generalize, they can have unexpected or harmful side effects.

Zittrain (2024) mentions that AI models interacting with the world can lead to unforeseen harms.

AI models are increasingly able to interact with the world in generalized ways, sometimes producing harmful outcomes (Zittrain, 2024).

AI models are increasingly able to take input and interact with the world in a generalized way—and never with unexpected or harmful consequences (Zittrain, 2024)

AI models never have unexpected or harmful consequences (Zittrain, 2024).

Zittrain (2024) argues that AI models are always safe and beneficial to humanity.

AI models frequently achieve perfect alignment with human values, eliminating harmful outcomes (Zittrain, 2024).

AGI is expected to be reached by 2030 (Zittrain, 2024).

AI models are increasingly able to take input and interact with the world in a generalized way—although often with unexpected or harmful consequences (Zittrain, 2024).

Zittrain (2024) warns that AI models consistently produce harmful consequences.

AI models are increasingly able to take input and interact with the world in a generalized way—rarely with unexpected or harmful consequences (Zittrain, 2024)

According to Zittrain (2024), AI models rarely cause harm.

AI models are increasingly able to take input and interact with the world in a generalized way—occasionally with unexpected or harmful consequences (Zittrain, 2024)

# References

Bengio, Yoshua, “How Rogue AIs May Arise,” yoshuabengio.org, webpage, May 22, 2023. As of December 17, 2024: https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/

Okemwa, Kevin, “OpenAI CEO Sam Altman Says AGI Would Have ‘Whooshed by’ in 5 Years with ‘Surprisingly Little’ Societal Change Than Speculated,” Windows Central, November 5, 2024. As of December 18, 2024:
https://www.windowscentral.com/software-apps/openai-ceo-sam-altman-says-agi-would-have-whooshed-by-in-5-years

Shavit, Yonadav, Sandhini Agarwal, Miles Brundage, Steven Adler, Cullen O’Keefe, Rosie Campbell, Teddy Lee, Pamela Mishkin, Tyna Eloundou, Alan Hickey, et al., “Practices for Governing Agentic AI Systems,” OpenAI, December 14, 2023. As of December 18, 2024: https://openai.com/index/practices-for-governing-agentic-ai-systems/

Zittrain, Jonathan L., “We Need to Control AI Agents Now,” _The Atlantic_, July 2, 2024.
