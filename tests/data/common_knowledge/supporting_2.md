9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

1/65

ArticleCan AI Scaling ContinueThrough 2030?We investigate the scalability of AI training runs. Weidentify electric power, chip manufacturing, data andlatency as constraints. We conclude that 2e29 FLOPtraining runs will likely be feasible by 2030.Can AI Scaling Continue Through 2030?Report9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

2/65

CitePublishedAug 20, 2024AuthorsJaime Sevilla, Tamay Besiroglu, Ben Cottier, Josh You, Edu Roldán, Pablo Villalobos, Ege ErdilResourcesSource CodeContentsIntroductionWhat constrains AI scaling this decadePower constraintsThe current trend of AI power demandPower constraints for geographically localized training runsPower constraints for geographically distributed trainingFeasibility of geographically distributed trainingModeling energy bottlenecksChip manufacturing capacityCurrent production and projectionsModeling GPU production and compute availabilityData scarcityMultimodalitySynthetic dataLatency wallLatency wall given intranode latenciesLatency wall given latencies between nodesHow can these latencies be reduced?9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

3/65

IntroductionIn recent years, the capabilities of AI models have significantlyimproved. Our research suggests that this growth in computationalresources accounts for a significant portion of AI performanceimprovements.1 The consistent and predictable improvements from scalinghave led AI labs to aggressively expand the scale of training, with trainingcompute expanding at a rate of approximately 4x per year.To put this 4x annual growth in AI training compute into perspective, itoutpaces even some of the fastest technological expansions in recenthistory. It surpasses the peak growth rates of mobile phoneadoption (2x/year, 1980-1987), solar energy capacity installation (1.5x/year,2001-2010), and human genome sequencing (3.3x/year, 2008-2015).Here, we examine whether it is technically feasible for the current rapidpace of AI training scaling—approximately 4x per year—to continuethrough 2030. We investigate four key factors that might constrain scaling:power availability, chip manufacturing capacity, data scarcity, and the“latency wall”, a fundamental speed limit imposed by unavoidable delays inAI training computations.What constraint is the most limiting?Will labs attempt to scale to these new heights?ConclusionAppendicesAppendix A: Summary of the extrapolative modelAppendix B: Fraction of total resources allocated to the largest training runAppendix C: Bandwidth constraintsAppendix D: Equivalence between multimodal and text dataAppendix E: Computing the largest possible training run given variablecommunication latency restrictionsAppendix F: Unprecedented economic growth could drive massive AIinvestmentNotes9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

4/65

Our analysis incorporates the expansion of production capabilities,investment, and technological advancements. This includes, among otherfactors, examining planned growth in advanced chip packaging facilities,construction of additional power plants, and the geographic spread of datacenters to leverage multiple power networks. To account for these changes,we incorporate projections from various public sources: semiconductorfoundries’ planned expansions, electricity providers’ capacity growthforecasts, other relevant industry data, and our own research.We find that training runs of 2e29 FLOP will likely be feasible by the end ofthis decade. In other words, by 2030 it will be very likely possible to trainmodels that exceed GPT-4 in scale to the same degree that GPT-4exceeds GPT-2 in scale.2 If pursued, we might see by the end of thedecade advances in AI as drastic as the difference between therudimentary text generation of GPT-2 in 2019 and the sophisticatedproblem-solving abilities of GPT-4 in 2023.Whether AI developers will actually pursue this level of scaling depends ontheir willingness to invest hundreds of billions of dollars in AI expansionover the coming years. While we briefly discuss the economics of AIinvestment later, a thorough analysis of investment decisions is beyond thescope of this report.Constraints to scaling training runs by 2030Training compute (FLOP)10339/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

5/65

For each bottleneck we offer a conservative estimate of the relevant supplyand the largest training run they would allow.3 Throughout our analysis, weassume that training runs could last between two to nine months,reflecting the trend towards longer durations. We also assume that whendistributing AI data center power for distributed training and chipscompanies will only be able to muster about 10% to 40% of the existingsupply.4Power constraints. Plans for data center campuses of 1 to 5 GW by 2030have already been discussed, which would support training runs rangingfrom 1e28 to 3e29 FLOP (for reference, GPT-4 was likely around 2e25FLOP). Geographically distributed training could tap into multiple regions’energy infrastructure to scale further. Given current projections of US datacenter expansion, a US distributed network could likely accommodate 2 to45 GW, which assuming sufficient inter-data center bandwidth wouldsupport training runs from 2e28 to 2e30 FLOP. Beyond this, an actor willingto pay the costs of new power stations could access significantly morepower, if planning 3 to 5 years in advance.Figure 1: Estimates of the scale constraints imposed by the most important bottlenecks toscale. Each estimate is based on historical projections. The dark shaded box correspondsto an interquartile range and light shaded region to an 80% confidence interval. Click onthe arrow to learn more.CC-BY1025102610271028102910301031103210Power constraintsChip production capacityData scarcity10,000times greaterMedian2e29 FLOP50,000times greaterMedian9e29 FLOP80,000times greaterMedian2e30 FLOP9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

6/65

Chip manufacturing capacity. AI chips provide the compute necessary fortraining large AI models. Currently, expansion is constrained by advancedpackaging and high-bandwidth memory production capacity. However,given the scale-ups planned by manufacturers, as well as hardwareefficiency improvements, there is likely to be enough capacity for 100MH100-equivalent GPUs to be dedicated to training to power a 9e29 FLOPtraining run, even after accounting for the fact that GPUs will be splitbetween multiple AI labs, and in part dedicated to serving models.However, this projection carries significant uncertainty, with our estimatesranging from 20 million to 400 million H100 equivalents, corresponding to1e29 to 5e30 FLOP (5,000 to 300,000 times larger than GPT-4).Data scarcity. Training large AI models requires correspondingly largedatasets. The indexed web contains about 500T words of unique text, andis projected to increase by 50% by 2030. Multimodal learning from image,video and audio data will likely moderately contribute to scaling, plausiblytripling the data available for training. After accounting for uncertainties ondata quality, availability, multiple epochs, and multimodal tokenizerefficiency, we estimate the equivalent of 400 trillion to 20 quadrillion tokensavailable for training by 2030, allowing for 6e28 to 2e32 FLOP trainingruns. We speculate that synthetic data generation from AI models couldincrease this substantially.Latency wall. The latency wall represents a sort of “speed limit” stemmingfrom the minimum time required for forward and backward passes. Asmodels scale, they require more sequential operations to train. Increasingthe number of training tokens processed in parallel (the ‘batch size’) canamortize these latencies, but this approach has a limit. Beyond a ‘criticalbatch size’, further increases in batch size yield diminishing returns intraining efficiency, and training larger models requires processing morebatches sequentially. This sets an upper bound on training FLOP within aspecific timeframe. We estimate that cumulative latency on modern GPUsetups would cap training runs at 3e30 to 1e32 FLOP. Surpassing this scalewould require alternative network topologies, reduced communicationlatencies, or more aggressive batch size scaling than currently feasible.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

7/65

Bottom line. While there is substantial uncertainty about the precise scalesof training that are technically feasible, our analysis suggests that trainingruns of around 2e29 FLOP are likely possible by 2030. This represents asignificant increase in scale over current models, similar to the sizedifference between GPT-2 and GPT-4. The constraint likely to bind first ispower, followed by the capacity to manufacture enough chips. Scalingbeyond would require vastly expanded energy infrastructure and theconstruction of new power plants, high-bandwidth networking to connectgeographically distributed data centers, and a significant expansion in chipproduction capacity.What constrains AI scaling this decadePower constraintsIn this analysis, we project the power requirements necessary to sustainthe current trajectory of scaling AI training. We then explore potentialstrategies to meet these power demands, including on-site powergeneration, local grid supply, and geographically distributed trainingnetworks. Our focus is on AI training runs conducted within the UnitedStates, examining the feasibility and constraints of each approach.5Data center campuses between 1 to 5 gigawatt (GW) are likely possible by2030. This range spans from Amazon’s 960 MW nuclear power contract inPennsylvania to the 5 GW campuses that OpenAI/Microsoft and SamAltman have been reported to be pursuing. Such campuses would supportAI training runs ranging from 1e28 to 3e29 FLOP, given expectedadvancements in the energy efficiency of ML GPUs.Scaling beyond single-campus data centers would involve geographicallydistributed training, which could utilize energy infrastructure acrossmultiple regions. Given current projections, a distributed training networkcould accommodate a demand of 2 to 45 GW, allowing for training runs of2e28 to 2e30 FLOP. Bandwidth could also constrain the largest trainingrun that could be done in such a network. Concretely, inter-data center9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

8/65

bandwidths of 4 to 20 Petabits per second (Ppbs), which are on trend forexisting data centers, would support training runs of 3e29 to 2e31 FLOP.This is likely high enough that bandwidth would not be a major obstaclecompared to securing the power supply.6Larger training runs are plausible: we expect the cost of the infrastructureneeded to power GPUs during a training run to be around 40% of the costof the GPUs themselves by 2030, and rapid expansion of the power supplyvia natural gas or solar power could be arranged within three to five yearsof a decision to expand—though this could be constrained byinfrastructure-level bottlenecks.The current trend of AI power demandAI model training currently consumes a small but rapidly growing portion oftotal data center power usage. Here, we survey existing estimates ofcurrent demand, extrapolates future trends, and compares theseprojections to overall data center and national power capacity.Large-scale AI training relies primarily on hardware accelerators,specifically GPUs. The current state-of-the-art GPU is Nvidia’s H100,7 whichhas a thermal design power (TDP) of 700W. After accounting for supportinghardware such as cluster interconnect and CPUs, and data center-leveloverhead such as cooling and power distribution, its peak power demandgoes up to 1,700W per GPU.8 Using the power demand per GPU, we can estimate the installed powerdemand for frontier models.  The recent Llama 3.1 405B model, with its4e25 FLOP training run, used a cluster of 16,000 H100 GPUs. Thisconfiguration required 27MW of total installed capacity (16,000 GPUs ×1,700W per GPU). While substantial—equivalent to the average yearlyconsumption of 23,000 US households9—this demand is still smallcompared to large data centers, which can require hundreds of megawatts.How much will this increase by the end of the decade? Frontier trainingruns by 2030 are projected to be 5,000x larger than Llama 3.1 405B,reaching 2e29 FLOP.10 However, we don’t expect power demand to scale byas much. This is for several reasons.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

9/65

First, we expect hardware to become more power-efficient over time. Thepeak FLOP/s per W achieved by GPUs used for ML training have increasedby around 1.28x/year between 2010 and 2024.11 If continued, we would see4x more efficient training runs by the end of the decade.Second, we anticipate more efficient hardware usage in future AI training.While Llama 3.1 405B used FP16 format (16-bit precision), there’s growingadoption of FP8 training, as seen with Inflection-2. An Anthropic co-founder has suggested FP8 will become standard practice in frontier labs.We expect that training runs will switch over to 8-bit by 2030, which will be~2x as power-efficient (e.g. The H100 performs around 2e15 FLOP/s at 8-bitprecision, compared to 1e15 FLOP/s at 16-bit precision).12Third, we expect training runs to be longer. Since 2010, the length oftraining runs has increased by 20% per year among notable models, whichwould be on trend to 3x larger training runs by 2030. Larger training rundurations would spread out energy needs over time. For context, Llama 3.1405B was trained over 72 days, while other contemporary models such asGPT-4 are speculated to have been trained over ~100 days. However, wethink it’s unlikely that training runs will exceed a year, as labs will wish toadopt better algorithms and training techniques on the order of thetimescale at which these provide substantial performance gains.Given all of the above, we expect training runs in 2030 will be 4x (hardwareefficiency) * 2x (FP8) * 3x (increased duration) = 24x more power-efficientthan the Llama 3.1 405B training run. Therefore, on-trend 2e29 FLOPtraining runs in 2030 will require 5,000x (increased scale) / 24x≈ 200x more power than was used for training of Llama 3.1 405B, for apower demand of 6 GW.These figures are still relatively small compared to the total installed powercapacity of the US, which is around 1,200 GW, or the 477 GW of power thatthe US produced on average in 2023.13 However, they are substantialcompared to the power consumption of all US data centers today, which isaround 20 GW,14 most of which is currently not AI related. Furthermore,facilities that consume multiple gigawatts of power are unprecedentedlymassive—energy-intensive facilities today such as aluminum smelters9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

10/65

demand up to around the order of a gigawatt of power, but not muchmore.15,16 In the following sections, we investigate whether such energy-intensive facilities will be possiblePower constraints for geographically localized training runsFor geographically localized training, whether done by a single data centeror multiple data centers in a single campus, there are two options forsupplying power: on-site generation, or drawing from (possibly multiple)power stations through the local electricity grid.Companies today are already pursuing on-site generation. Meta bought therights to the power output of a 350MW solar farm in Missouri and a 300MWsolar farm in Arizona.17 Amazon owns a data center campus in Pennsylvaniawith a contract for up to 960 megawatts from the adjoining 2.5 GW nuclearplant. The primary motivation behind these deals is to save on gridconnection costs and guarantee a reliable energy supply. In the comingyears such data centers might allow unprecedentedly large training runs totake place—960 MW would be over 35x more power than the 27MW required for today’s frontier training runs.Could one acquire even more power through on-site generation? Currently,there are at least 27 power plants with capacity greater than 2.5 GW in theUS,18 ranging in size up to the Grand Coulee 6.8GW hydroelectric plant inWashington. However, a significant portion of the power capacity fromexisting plants is likely already committed through long-termcontracts.19 This limited availability of spare capacity suggests that existingUS power plants may face challenges in accommodating large-scale on-site generation deals. The scarcity of spare energy capacity also breedsdisputes. For example, Amazon’s bid for 960 MW of on-site nuclear poweris challenged by two utilities seeking to cap Amazon at its current 300 MWpurchase. They argue this arrangement evades shared grid costs; suchdisputes may also inhibit other on-site power deals.More large-scale plants might be constructed in the coming years, but fewhave been built recently, and the most recent >3 GW power stations tookaround five years to build.20 It seems unlikely that any already-planned US9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

11/65

power stations will be able to accommodate an on-site data center in the>3 GW range by 2030.21 Instead, moving to larger scales will likely requiredrawing electricity from the grid. As a proxy, we can look at data center consumption trends ingeographically localized areas. For instance, Northern Virginia is the largestdata center hub in the US, housing nearly 300 data centers that areconnected to 5 GW of power in peak capacity.22 The largest NorthernVirginia electricity provider, Dominion, expects their data center load toincrease 4x in the next fifteen years, for an implied 10% yearly growth rate.If Dominion and other regional providers stick to similar expansion plans,by 2030 we might expect data center power capacity in Northern Virginia togrow to around 10 GW.23Some companies are investigating options for gigawatt-scale data centers,a scale that seems feasible by 2030. This assessment is supported byindustry leaders and corroborated by recent media reports. The CEO ofNextEra, the largest utility company in the United States, recentlystated that while finding a site for a 5-gigawatt AI data center would bechallenging, locations capable of supporting 1-gigawatt facilities do existwithin the country. It is also consistent with a media report indicating thatMicrosoft and OpenAI are tentatively planning an AI data center campusfor 2028 dubbed Stargate that will require “several gigawatts of power”,with an expansion of up to 5 GW by 2030.24 In sum, current trajectories suggest that AI training facilities capable ofaccommodating 2 to 5 GW of power demand are feasible by 2030. Thisassessment is based on three key factors: the projected growth of datacenter power capacity, exemplified by Northern Virginia’s expectedincrease from 5 GW to 10 GW; ambitious industry plans for gigawatt-scaledata centers, such as the rumored Stargate campus; and utility companyassessments indicating that 1 to 5-gigawatt facilities are viable in select USlocations. For context, a 5 GW power supply such as the rumored Stargatecampus would allow for training runs of 2e29 FLOP by 2030, accounting forexpected advances in energy efficiency, and an increase in trainingduration to over 300 days.25 Training networks powered by co-located9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

12/65

power plants or local electricity networks are unlikely to exceed 10 GW—asthis would come close to the total projected power demand across all datacenters in Northern Virginia. Power constraints for geographically distributed trainingDistributing AI training beyond a single data center can help circumventlocal power constraints. Inter-data center distributed training involvesspreading workloads across multiple data centers, which may or may notbe in close proximity. This method has likely been used for large models likeGemini Ultra, allowing access to more hardwareresources.26 Geographically distributed training extends this conceptacross wider areas, potentially tapping into separate electricity grids. Majortech companies are well-positioned for this approach, with data centersalready spread across multiple regions. For example, Google operates datacenters in 15 different U.S. states.27 This approach could enable larger-scaletraining operations by accessing a broader pool of power resources.How much power could distributed data center networks access? As withlocal data center networks, we ground our discussion in historical trends,supplier forecasts and third-party projections of data center power growth.In a later section we discuss factors that would affect the feasibility of amajor expansion in the US’s overall power supply, which could unlock evenmore power for data centers.The potential for US data centers to access electricity is substantial andgrowing. To accurately assess this capacity, it’s crucial to distinguishbetween two key metrics: the average rate of actual energy consumption,which accounts for downtime and fluctuations, and the total peak capacityfor which data centers are rated. We estimate that average powerconsumption across US data centers is over 20 GW today.28 Dominion hassaid that the data centers they serve demand 60% of their capacity onaverage, and estimates from experts we spoke to suggest that data centersconsume around 40-50% of their rated capacity, on average. This suggestsan overall capacity of 33 to 50 GW, or ~40 GW as a central estimate.29 Inaddition, according to SemiAnalysis’ data center industry model, total datacenter IT capacity in North America (the vast majority of which is in the US)9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

13/65

was ~36 GW at the end of 2023 and will be ~48 GW at the end of 2024,which is consistent with this estimate.30Figure 2: Reported and planned total installed IT capacity of North America data centers viaSemiAnalysis’ data center industry model. Important note: to find total capacity, we mustmultiply these figures by PUE, which ranges from 1.2x for AI datacenters to 1.5x for otherdatacenters.The potential for rapid expansion in data center power capacity issignificant, as evidenced by multiple sources and projections.  Historicaldata from SemiAnalysis indicates tracked data center capacity grew at anannual rate of ~20% between 2019 and 2023, per (see figure 2). Plannedexpansions in 2024 and 2025 aim to accelerate this, achieving 32% yearlygrowth if completed on time.We can also look at growth projections from utilities companies to estimatea feasible growth rate for the overall data center industry. In NorthernVirginia, Dominion is planning for a 10-15% annual growth rate31 in datacenter power in the coming years, following 24% annual demandgrowth from 2017 to 2023. NOVEC, another Virginia utility, expects 17%yearly growth in the coming years.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

14/65

Finally, other independent estimates are consistent with a ~15% annualgrowth rate, such as from Goldman Sachs, which projects that data centerpower consumption will grow at an annual rate of 15% to 400 TWh in 2030(for an average demand of ~46 GW), and from the Electric Power ResearchInstitute (EPRI), which considers a 15% growth rate if there is a rapidexpansion of AI applications.Overall, an annual growth rate of 10-30% seems achievable. Acentral estimate of 15% growth would imply that US data center capacitycould grow from 40 GW to up to 90 GW by 2030, or an increase of 50 GW.Note again that we are using a range of projections of actual growth toground estimates of feasible growth, so this figure is arguably conservative.Given power capacity for all data centers, how much power would beavailable for AI?   Currently, the majority of US data centers are dedicated tonon-AI uses such as internet services and cloud computing. For instance,SemiAnalysis tracks 3 GW of installed capacity across AI data centers inNorth America by the end of 2023. This corresponds to ~8% of total datacenter capacity.32 However, the share of power demand from AI datacenters is on the rise, and we expect the AI power capacity share tobecome even larger in the coming years.Existing forecasts of the annual growth in power demand for non-AI datacenters center around 8% to 11%.33 At a 8% growth rate, demand for non-AIapplications would increase from around 37 GW today to around 60 GW by2030, leaving 90 GW - 60 GW ≈ 30 GW capacity for AI data centers. Thiswould result in roughly a 30 GW / 3 GW ≈ 10x expansion in AI installedcapacity, or roughly 47% annual growth on AI installed powercapacity.34 This projection assumes a fixed allocation of growth to non-AIapplications. However, if AI applications prove more profitable orstrategically important, cloud providers could potentially reallocateresources, leading to even higher growth in AI installed power capacity atthe expense of non-AI expansion.Finally, we estimate how much of this capacity can be dedicated to a singletraining run. We must account for the fact that this added power capacitywill likely be shared between different actors, such as Microsoft, Google,9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

15/65

Amazon and so on. Our best guess is that the company with the largestshare might get around 33% of the power capacity for AI datacenters. Companies can front-load their capacity for training, leading tomost of their capacity at the time of starting a large training run to bededicated to training, perhaps by as much as 80%. In total, this means that33% x 80% = 26% of the AI data center capacity might be used in a singletraining run.35Given our estimates, the most well-resourced AI company in the US willlikely be able to orchestrate a 30 GW x 26% ≈ 8 GW distributed training runby 2030. After accounting for uncertainties on the relevant growth ratesand current capacities, we end up with a conservative estimate of 2 to 45GW for the largest supply that a developer will be able to muster fordistributed training, which would allow for training runs between 2e28 to2e30 FLOP (see figure 3 in a later section). For context, our earlier analysissuggested that single-campus facilities might achieve 2 to 5 GW capacityby 2030. The upper end of our distributed training estimate (45 GW)significantly exceeds this single-campus projection, indicating thepotential for distributed training to overcome power bottlenecks.Feasibility of geographically distributed trainingGeographically distributed training runs, which spread workloads acrossmultiple data centers to alleviate power constraints, are likely to betechnically feasible at the scale projected in our analysis. This approachbuilds upon existing practices in AI model training, where computationsare already massively parallelized across numerous GPUs. The fundamentalstructure of AI training facilitates geographical distribution: datasets aredivided into batches, with model weight updates occurring only once perbatch. In a distributed setup, these batches can be split across variouslocations, requiring data centers to synchronize and share gradientupdates only at the conclusion of each batch.Evidence for the viability of this approach exists in current practices. Forinstance, Google’s Gemini Ultra model was reportedly trained acrossmultiple data centers, demonstrating the practicality of geographicallydispersed training.36 While the exact geographic spread of the data centers9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

16/65

used for Gemini Ultra is unclear, its training provides a concrete example oflarge-scale distributed operations.The feasibility of widely dispersed data centers in distributed training isconstrained by latency. In a scenario where major U.S. data centers areconnected by an 11,000 km fiber optic loop (a high-end estimate), thecommunication latency would be approximately 55ms.37 Synchronizationwould require two round trips down the network, taking 110ms. This is usinga travel speed that is two-thirds the speed of light, so this latency cannotbe reduced as long as we are doing fiber optic communication. So if atraining run is completed within 300 days, it could involve at most 300 days/ 110ms = 240 million gradient updates.We are uncertain how large batches can be without compromising trainingeffectiveness. We will assume it to be 60 million tokens—which isspeculated to match the largest batch size achieved by GPT-4 duringtraining. This would allow for ~1e16 tokens (240M batches x 60Mtokens/batch) to be seen during training, which under Chinchilla-optimalscaling would allow for a ~6e31 FLOP training run.38 In other words, latencyis not likely to be the binding constraint, even when pessimisticallyassuming a data center network involving very distant data centers.Beyond latency, bandwidth also influences the feasibility of large-scaledistributed training. Current data center switch technology, exemplified bythe Marvell Teralynx 10, provides insight into achievable bandwidth.This data center switch supports 128 ports of 400 Gps each, for a totalbandwidth of 51.2 Tbps.39 Transmitting the gradient updates for a 16Tparameter model at 8-bit precision using a standard two-stage ring all-reduce operation would then take 2 x 16T x 8 bit / 51.2 Tbps = 4.9 secondsper trip. Adding 110ms of latency per all-reduce as before, the total time perall-reduce would be 5 seconds in total. Given Chinchilla scaling, this modelsize would maximize the scale of training that can be accomplished inunder 300 days of training, leading to a 3e28 FLOP training run.40However, achievable bandwidths are likely to be much higher than whatcan be managed by a single Teralynx 10 ethernet switch. First, linksbetween data centers pairs can be managed by multiple switches and9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

17/65

corresponding fibers, achieving much larger bandwidths. For instance,each node in Google’s Stargate network featured 32 switches managingexternal traffic. In a ring all-reduce setup, a 32-switch data center coulddedicate 16 switches to manage the connection with each of its twoneighbors. Given the precedent of Google’s B4 network, we think thatswitch arrangements of 8 to 32 switches per data center pair should beachievable.41Second, better switches and transceivers will likely exist in the future,increasing the achievable bandwidth. The broader trend of ASIC switchessuggests a 1.4 to 1.6x/year increase in bandwidth,42 which would result in380 to 850 Tbps ethernet switches by the end of the decade.43Our final estimate of the achievable inter data center bandwidth by 2030 is4 to 20 Pbps, which would allow for training runs of 3e29 to 2e31 FLOP. Inlight of this, bandwidth is unlikely to be a major constraint for a distributedtraining run compared to achieving the necessary power supply in the firstplace.Expanding bandwidth capacity for distributed training networks presents arelatively straightforward engineering challenge, achievable through thedeployment of additional fiber pairs between data centers. In the context ofAI training runs potentially costing hundreds of billions of dollars, thefinancial investment required for such bandwidth expansion appearscomparatively modest.44Modeling energy bottlenecksWe conclude that training runs in 2030 supported by a local power supplycould likely involve 1 to 5 GW and reach 1e28 to 3e29 FLOP by 2030.Meanwhile, geographically distributed training runs could amass a supplyof 2 to 45 GW and achieve 4 to 20 Pbps connections between data centerpairs, allowing for training runs of 2e28 to 2e30 FLOP.45 All in all, it seemslikely that training runs between 2e28 to 2e30 FLOP will be possible by2030.46 The assumptions behind these estimates can be found in Figure 3below.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

18/65

Most power-intensive training setups feasible by 2030and the training runs they would enableFigure 3: Projected power consumption of local and distributed data center networksetups, alongside the scale of the largest training run they would support, accounting forenergy efficiency improvements and bandwidth and latency constraints.Learn more about our assumptions.Median50% CI80% CIDatacenter power consumption in 2030 (GW)10-210-1100101102103Largest localtraining runLargest distributedtraining runMedian50% CI80% CILargest training run in 2030 (FLOP)102510261027102810291030103110321033Largest localtraining runLargest distributedtraining runBandwidthconstraint9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

19/65

How much further could power supply be scaled?At this point, it is unclear to us how far power provision for datacenters could scale if pursued aggressively. So far, we have groundedour discussion in the existing power supply for data centers as wellas growth projections from utilities, and the results of our modelreflect these estimates. How could these numbers change if there isan unprecedented investment in growing the power supply?47Building new power plants seems reasonably affordable and…View moreChip manufacturing capacityAI chips, such as GPUs, provide the compute required to train AI modelsand are a key input in AI scaling. Growth in GPU clusters has been the maindriver of compute growth in the past few years, and higher performance,lower latency, higher memory bandwidth GPUs make it feasible to do everlarger training runs. AI scaling could therefore be constrained by thenumber of state-of-the-art GPUs that chipmakers can produce.We model future GPU production and its constraints by analyzingsemiconductor industry data, including projected packaging capacitygrowth, wafer production growth, and capital expenditure on fabricationplants (fabs). Our projections indicate that GPU production through 2030 isexpected to expand between 30% to 100% per year, aligning with CoWoSpackaging and HBM production growth rates.In our median projection, we expect enough manufacturing capacity toproduce 100 million H100-equivalent GPUs for AI training, sufficient topower a 9e29 FLOP training run. This estimate accounts for GPUs beingCC-BY9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

20/65

distributed among multiple AI labs and partially used for model serving.However, this projection has significant uncertainty, primarily due tounknowns in advanced packaging and high-bandwidth memory capacityexpansions. Our estimates range from 20 million to 400 million H100equivalents, potentially enabling training runs between 1e29 and 5e30FLOP (5,000 to 250,000 times larger than GPT-4).Current production and projectionsRecent years have seen rapid growth in data center GPU sales. Nvidia,which has a dominant market share in AI GPUs, reportedly shipped 3.76million data center GPUs in 2023, up from 2.64 million units in 2022.58 Bythe end of 2023, 650,000 Nvidia H100 GPUs were shipped to major techcompanies. Projections for 2024 suggest a potential threefold increase inshipments, with expectations of 1.5 to 2 million H100 units shipped. Thisvolume would be sufficient to power a 6e27 FLOP training run.59However, if we extrapolate the ongoing 4x/year trend in trainingcompute to 2030, we anticipate training runs of around 2e29 FLOP. Atraining run of this size would require almost 20M H100-equivalentGPUs.60 If we suppose that at most around 20% of total production can benetted by a single AI lab, global manufacturing capacity would need toreach closer to 100M H100-equivalent GPUs by 2030. This far exceedscurrent production levels and would require a vast expansion of GPUproduction.61TSMC, the company that serves as Nvidia’s primary chip fab, faces severalchallenges in increasing production. One key near-term bottleneck is chippackaging capacity—particularly for TSMC’s Chip-on-wafer-on-Substrate(CoWoS) process, which is Nvidia’s main packaging method for their latestGPUs. This packaging process combines logic units with high-bandwidthmemory (HBM) stacks into ready-to-use AI chips. Packaging is difficult toscale up rapidly, as new facilities require complex equipment from manyvendors. Constructing these facilities also requires specialized training forpersonnel. These constraints have limited TSMC’s AI chip output, despitestrong demand from customers like Nvidia.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

21/65

TSMC is directing significant efforts to address this constraint. Thecompany is rapidly increasing its CoWoS packaging capacity from 14,000-15,000 wafers per month in December 2023 to a projected 33,000-35,000wafers per month by the end of 2024.62 To further expand this capacity,TSMC opened a new fab in 2023, Advanced Backend Fab 6. At full capacity,this fab could process up to 83,000 wafers per month, which would morethan double TSMC’s advanced packaging volume.63 TSMC has alsoannounced plans to increase its packaging capacity by 60% annuallythrough 2026. Recent scale-ups of CoWoS capacity have ranged from30% to 100% annually. If this trend continues, the number of dies of a fixedsize that can be produced will likely increase at a similar rate.64,65The production of HBM chips themselves is another significant constrainton GPU manufacturing. HBM chips are nearly sold out until 2026. WhileHBM volume is expected to increase 2-3x from 2023 to 2024, much of thisgrowth comes from reallocating capacity from DRAM, which is anotherclass of lower-end memory chips. SK Hynix, the current HBM leader andNvidia’s main supplier, projects a 60% annual growth in HBM demand in themedium to long-term (likely referring to revenue), while one analyst firmestimates 45% annual growth in production volume from 2023 to 2028.HBM production and TSMC’s CoWoS packaging capacity, two keyconstraints in GPU manufacturing, are projected to expand at similar ratesin the coming years. Based on TSMC’s announced plans and recent growthtrends in CoWoS capacity, which have ranged from 30% to 100% annually,we estimate GPU production will grow at a similar rate in the near term.Despite the substantial growth in GPU production, wafer production itselfis not likely to be the primary limiting factor. Currently, data center GPUsrepresent only a small portion of TSMC’s total wafer production. Our centralestimate of TSMC’s current production capacity for 5nm and 3nm processnodes is 2.2M wafers per year as of early 2024.66 The projected 2 millionH100 GPUs for 2024 would only consume about 5% of the 5nm node’scapacity.67 Even with the projected growth rates, it’s unlikely that GPUmanufacturing will dominate TSMC’s leading-edge capacity in the9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

22/65

immediate future. Instead, the main constraints on expanding GPUproduction appear to be chip packaging and HBM production.However, it is plausible that GPU manufacturing could eventually come todominate TSMC’s leading-edge nodes. There is precedent for such ascenario; in 2023, Apple absorbed around 90% of TSMC’s 3nm production.Given the high profit margins on AI chips, Nvidia could potentially outbidcompetitors like Apple and Qualcomm for TSMC’s advanced wafercapacity. While we think this is plausible, this scenario is not featured in ourmainline analysis.Modeling GPU production and compute availabilityTSMC forecasted AI server demand to grow at 50% annually over the nextfive years. Given TSMC’s historical 5 percentage point yearly increase inoperating margins, which investors expect to continue due to price hikes,we estimate actual GPU volume growth at around 35% per year.68 This isconservative compared to other projections: AMD expects 70% annualgrowth for data center chips through 2027, implying about 60% annualGPU volume growth assuming similar price increases.69 These moreaggressive estimates align closely with near-term CoWoS packaging andHBM production scale-up projections discussed above, lending themcredibility. We take this range of estimates into account, and projectproduction of GPU dies to expand somewhere between 30% and 100% peryear.We expect that there will be enough wafer capacity to sustain thisexpansion. TSMC’s historical trends show capex growth of 15%annually and wafer capacity expansion of 8% yearly from 2014 to2023.70 TSMC might increase its capex dedicated to expanding GPUproduction and substantially expand the production of GPU-earmarked-wafers, packaging and other parts of the production. If TSMC acceleratesits capex growth to match their expected growth in the AI server market of50% annual growth, the historical relationship between input and outputgrowth suggests that total wafer capacity could expand by 27% peryear.71 Overall, this points to a growth rate of leading-edge waferproduction of between 5% and 20% per year.9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

23/65

We have appreciable uncertainty about current leading-edge waferproduction, and assume that this is somewhere between 100k to330k/month. At 5 to 20% yearly growth, we project the total stock ofleading-edge wafers produced by 2030 to be between 10M and 37M. Basedon TSMC’s and others’ projections, we expect around 20% of these wafersto be dedicated to producing data center GPUs.  These projections indicate that 2e30 to 4e31 FLOP/year worth of globalstocks of H100 equivalents will be produced in aggregate. Of course, onlysome fraction of this will be dedicated to a single training run, sinceindividual labs will only receive some small fraction of shipments, labs willearmark their GPUs to inference and other experiments, and training runswill likely last less than a year. At current rates of improvements inhardware and algorithms and growing budgets for AI, training runs willlikely not exceed six months if hardware or software progress does notslow. We assume that training runs will last around 2 to 9 months; on thehigher end if progress in hardware and software stalls, and the lower end ifprogress accelerates relative to today.It is likely that AI chips will be distributed across many competing labs,with some lab owning some non-trivial fraction of global compute stocks.For instance, Meta reportedly bought one fourth of H100 shipments tomajor companies in 2023. We estimate that recently, the share ofdatacenter GPUs owned by a single lab at any point in time might besomewhere between 10% and 40%.Of this allocation, some fraction will likely be tied up with serving modelsand unavailable for training. It is difficult to know what fraction this mightbe. However, we can use a simple heuristic argument. A simpleanalysis suggests AI labs should allocate comparable resources to bothtasks. If this holds, and compute for training continues to grow 4x per year,then we should expect about 80% of the total available compute to be usedto train new models.72Putting all this together leads us to the following picture. On the mediantrajectory, about 100M H100-equivalents could, in principle, be dedicated totraining to power an 9e29 FLOP training run. However, this projection9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

24/65

carries significant uncertainty, with our estimates ranging from 20 million to400 million H100-equivalents, corresponding to 1e29 to 5e30 FLOP. Toestablish an upper bound, we considered a hypothetical scenario whereTSMC’s entire capacity for 5nm and below is devoted to GPU production,from now until 2030. In this case, the potential compute could increase byan order of magnitude, reaching 1e30 to 2e31 FLOP. This upper limit, basedon current wafer production projections, illustrates the maximum possibleimpact on AI training capabilities if existing constraints in packaging, HBMproduction, and wafer allocation were fully resolved. Figure 4 belowillustrates these estimates, and lists the assumptions behind them.Projected largest fleet of chips dedicated to training in2030, and the largest training runs they would enableMedian50% CI80% CINumber of H100 Equivalents1051061071081091010Projected TSMC capacityFull TSMC CapacityMedian50% CI80% CILargest training run in 2030 (FLOP)10329/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

25/65

Data scarcityScaling up AI training runs requires access to increasingly large datasets.So far, AI labs have relied on web text data to fuel training runs. Since theamount of web data generated year to year grows more slowly than thedata used in training, this will not be enough to support indefinite growth.In this section, we summarize our previous work on data scarcity, andexpand it by estimating further possible gains in scale enabled bymultimodal and synthetic data.The largest training datasets known to have been used in training are onthe order of 15 trillion tokens of publicly available text and code data.73 Weestimate that the indexed web contains around 500 trillion tokens afterdeduplication, 30 times more data than the largest known trainingdatasets. This could be as low as 100T if only looking at already compiledcorpora like CommonCrawl, or as high as 3000T if also accounting forprivate data.74 Since the Chinchilla scaling laws suggest that one ought to scale updataset size and model size proportionally, scaling up training data by afactor of 30x by using the entirety of the indexed web would enable AI labsFigure 4: Distribution of H100-equivalent GPUs and FLOP available for the largest AItraining run in 2030 under different scenarios. “Projected TSMC capacity” estimatesTSMC’s capacity for GPU production based on historical trends and projections, while “FullTSMC capacity” is a hypothetical where 100% of TSMC’s leading-edge wafer capacity goesto GPU production.Learn more about our assumptions.CC-BY102510261027102810291030103110Projected TSMC capacityFull TSMC Capacity9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

26/65

to train models with 30x more data and 30x more parameters, resultingin 900x as much compute, i.e. up to 8e28 FLOP if models are trained to beChinchilla-optimal.75,76,77If the recent trend of 4x/year compute scaling continues, we would run intothis “data wall” for text data in about five years. However, data from othermodalities and synthetic data generation might help mitigate thisconstraint. We will argue that multimodal data will result in effective datastocks of 450 trillion to 23 quadrillion tokens, allowing for training runs of6e28 to 2e32 FLOP. Furthermore, synthetic data might enable scaling muchbeyond this if AI labs spend a significant fraction of their compute budgetson data generation.78Copyright restrictionsPublished text data may be subject to copyright restrictions thatprohibit its use in training large language models without permission.While this could theoretically limit the supply of training data, severalfactors mitigate this concern in practice. The primary consideration isthe ongoing legal debate surrounding whether the inclusion ofpublished text in a general-purpose model’s training data constitutes“fair use”. However, even if this debate is settled in favor of copyrightholders, there are further practical considerations that complicatethe enforcement of such restrictions.Many large repositories of public web data, such as Blogspot, allow…View moreMultimodalityAI labs could leverage other data modalities such as image orvideo.79 Current multimodal foundation models are trained on datasetswhere 10-40% is image data.80 This data is used to allow models tounderstand and generate images. Given the usefulness of multimodalunderstanding, we expect future datasets to include a significant portion of9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

27/65

non-text data purely for this purpose. That said, to significantly expand thestock of data, the portion of multimodal data would have to become fargreater than that of text data.Audio, image or video modeling will be valuable enough on its own that AIlabs will scale pure audiovisual training. Strong visual abilities could enablemodels to act as assistants embedded within workflows to organizeinformation or operate a web browser. Models that have fluent, fast,multilingual speech abilities are likely to enable much improved personalvoice assistant technology, realtime translation, customer service and morefluid interactions compared to text-only. While current vision models usemuch less compute than language models,81 in a scenario where text datais a bottleneck but image data is plentiful, AI labs might start dedicatingmore resources to image models.Additional modalities like protein sequences or medical data are alsovaluable. However, the stock of such data is unlikely to be large enough tosignificantly expand the stock of available training data.82Multimodal data can further aid language understanding in various ways.Textual data can be transcribed from audio, image and video data, whichcould further expand the stock of text-related data.83 More speculatively,non-text data may improve language capabilities through transfer learningor synergy between modalities. For example, it has been shown thatcombining speech and text data can lead to improved performancecompared to single-modality models, and it is suggested that suchsynergies improve   with scale. However, research on transfer learningbetween modalities is scarce, so we can’t conclude with certainty thattransfer learning from multimodal data will be useful.How much visual data would be available for training if one of thesescenarios came to pass? The internet has around 10 trillion seconds ofvideo, while the number of images may also be close to 10T.84 It’schallenging to establish a rate of equivalence between these modalitiesand text data. Current multimodal models such as Chameleon-34B encodeimages as 1024 tokens, but we expect that as multimodal tokenizers andmodels become more efficient this will decrease over time. There are9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

28/65

examples of efficient encoding of images with as few as 32 tokens, whichafter being adjusted by typical text dictionary size would result in 22tokens per image.85 We take 22 tokens per image and second of video as acentral guess, which means that image and video multimodality wouldincrease the effective stock of data available for training by roughly 400Ttokens.86 This suggests that image and video content might eachcontribute as much as text to enable scaling. This would allow for trainingruns ten times larger than if trained purely on text data.Moreover, there are probably on the order of 500B-1T seconds of publiclyavailable audio on the internet.87 Neural encoders can store audio at <1.5kbps while being competitive with standard codecs at much higher bitrate.This corresponds to <100 language-equivalent tokens per second of audio.So it seems likely that total stored audio is on the order of 50-100T trilliontokens, not far from text and image estimates.88 Hence, this would probablynot extend the stock of data by a large factor.After adding the estimates from all modalities and accounting foruncertainties in the total stock of data, data quality, number of epochs, andtokenizer efficiency, we end up with an estimate of 400 trillion to 20quadrillion effective tokens available for training, which would allow fortraining runs of 6e28 to 2e32 FLOP by 2030 (see Figure 5).Given how wide this interval is, it may be useful to walk through how thehigh end of the range could be possible. Note that these numbers aremerely illustrative, as our actual confidence interval comes from a MonteCarlo simulation based on ranges of values over these parameters.A high-end estimate of the amount of text data on the indexed web is twoquadrillion tokens (Villalobos et al, 2024). Meanwhile, a high-end estimateof the number of images and seconds of video on the internet is 40 trillion.If we also use a higher-end estimate of 100 tokens per image or video-second, this would mean four quadrillion visual tokens, or six quadrilliontext and visual tokens. If we also assume that this stock of data doubles by2030, 80% is removed due to quality-filtering (FineWeb discarded ~85% oftokens), and models are trained on this data for 10 epochs, this would lead9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

29/65

to an effective dataset size of ~20 quadrillion tokens. See Figure 5 for a fulllist of these parameters and our reasoning for the value ranges we chose.Projected data stocks in 2030 and the largest trainingruns they would enableMedian50% CI80% CIEffective data stock in 2030 (tokens)10121013101410151016TextImageVideoAudMedian50% CI80% CILargest training run in 2030 (FLOP)10251026102710281029103010311032TextMultimodal(text + image + video + audio)9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

30/65

Synthetic dataIn our projections we have only considered human-generated data. Couldsynthetic data generation be used to greatly expand the data supply?Several important milestones in machine learning have been achievedwithout relying on human data. AlphaZero and AlphaProof learned to playgames and solve geometry problems respectively, matching or surpassinghuman experts by training purely on self-generated data. Language modelsfine-tuned on synthetic data can improve their ability to code and answerreasoning questions. Small LLMs trained on carefully curated syntheticdata can achieve comparable or superior performance with significantlyfewer parameters and less training data compared to larger models trainedon web-scrape text. Large-scale frontier language models like Llama 3.1 usesynthetic data to augment capabilities in areas where collecting high-quality human-annotated data might be challenging or costly, such as long-context capabilities, multilingual performance, and tool use capabilities.One key reason to expect it should be possible to spend compute togenerate high-quality synthetic data is that it’s often easier to verify thequality of an output than it is to generate it. This principle most clearlyapplies in domains where we can create explicit correctness or qualitysignals. For example, in coding tasks, we can check if generated codepasses unit tests or produces correct outputs for sample inputs. Inmathematics, we can detect logical or arithmetic mistakes and correctthem.This process enables developers to use compute to generate numerouscandidate solutions. They can then systematically verify the correctness orquality of each generated solution, keeping only the high-quality exampleswhile discarding the subpar ones. This approach can computationallycreate datasets filled with high-quality, synthetic examples. For these tasks,Figure 5: Projections for the amount of data of each modality and the largest efficienttraining run they would allow.Learn more about our assumptions.CC-BY9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

31/65

one can expend more inference compute to generate outputs of higherquality.The verification-easier-than-generation principle may extend beyondcoding to various other domains. For instance, it’s often easier to review aresearch paper for quality and novelty than to write an original paper.Similarly, evaluating the coherence and plausibility of a story is typicallyless challenging than writing an engaging story from scratch. In thesecases, while traditional symbolic systems might struggle with verification,modern AI systems, particularly large language models, havedemonstrated evaluation capabilities comparable with human verifiers. Thissuggests that AI-driven verification could enable creating high-qualitysynthetic data in these complex domains.There are additional mechanisms which can be used to generate high-quality synthetic data. For example, it is often the case that a model cannotproduce high-quality outputs directly, but it can produce them bycombining several smaller steps. This is the key idea behind chain-of-thought prompting, which can be used to teach models increasinglycomplex arithmetic by bootstrapping from simpler examples.Generating endless dataEven if it is technically possible to generate useful synthetic data fora wide range of tasks, the computational overhead of generationmight preclude its usage in practice. We can try to estimate howmuch additional compute would be needed to scale models usingsynthetic data, compared to a baseline of scaling natural datasets.Suppose that we have access to a frontier model which we will use…View moreThere are several obstacles for using synthetic data. The first is thepossibility of model collapse: over-reliance on synthetic data might cause adegeneration or stagnation of capabilities. It’s unclear if the self-correction9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

32/65

mechanisms we have introduced are enough to avoid this outcome,although there are promising signs.Increasing compute allocation for data generation can enhance synthetictraining data quality through two types of approaches: generating manycandidates then filtering for quality, and employing compute-intensivemethods like chain-of-thought reasoning to produce superior outputsdirectly. However, this strategy may face diminishing returns ascompute investment grows. When verification or quality estimationprocesses are imperfect, improvements in data quality may plateau despiteadditional compute allocation.90Synthetic data is already useful for domains where verification isstraightforward such as math and coding or in a small set of domainswhere collecting high-quality human-annotated data might be challengingor costly, such as tool use, long-context data or preference data. Based onthis success, and the intuitions we’ve discussed, we find it plausible thathigh-quality synthetic data generation is possible in a wide range of fields,beyond what has been demonstrated until now, but this is still uncertain. Inthis case, data availability might not pose a constraint on scaling, as morecould be generated on demand by spending enough compute.We expect synthetic data to likely be useful for overcoming databottlenecks. However, the research on the topic is nascent and the state ofexisting evidence is mixed, and so in this article we conservatively rely onestimates from multimodal data, excluding all types of synthetic data.Latency wallAnother potential constraint to AI scaling is latency. There is a minimumtime required for a model to process a single datapoint, and this latencygrows with the size of the model. Training data is divided into batches,where the data in a batch can be processed in parallel, but there are limitsto how large these batches can be. So a training run must last at least aslong as the time to process one batch, multiplied by the number of trainingbatches (training dataset size divided by the batch size). Given a finite9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

33/65

training run duration, this dynamic limits the size of a model and how muchdata it can be trained on, and thus the total size of the training run.91This constraint does not pose much of an issue for today’s training runsbecause typical latencies are very small. However, it could becomesubstantially more important in larger training runs as the minimum latencyincreases with model size due to the sequential nature of operations acrosslayers.Training runs can partially manage this latency issue by increasing thebatch size, allowing more data to be processed in parallel. In particular,increasing the batch size improves the convergence of stochastic gradientdescent, at the cost of more computational resources. This enables one tospeed up training at the cost of more compute per batch size, thoughwithout substantially increasing the overall compute needed for training.However, beyond a “critical batch size”, further batch increases yielddrastically diminished returns per batch. It is therefore not efficient to scalebatches indefinitely, and training a model on ever-larger datasets requiresincreasing the number of batches that need to be processed in sequence.To get a quantitative sense of the size of this bottleneck, we investigate therelevant sources of latency in the context of training large transformermodels. Given batch sizes of 60 million tokens (speculated to be the batchsize of GPT-4), we arrive at training runs between 2e30 to 2e32 FLOP,which would incur at least 270 to 400 microseconds (µs) of NVLINK andInfiniband communication latency per layer.However, this may be an underestimate because we expect that the criticalbatch size likely scales with model size. Under a speculative assumptionthat batch size can be scaled as roughly the cube root of model size, weestimate the feasibility of training runs around 3e30 to 1e32 FLOP, whichwould incur at least 290 to 440 µs of latency with modern hardware.92Latency wall given intranode latenciesWe first focus our analysis on intranode latencies, meaning latenciesassociated with a single node (i.e. a server) hosting multiple GPUs. In thiscase there are two types of latency that are most pertinent: the kernel9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

34/65

latency captures how long a single matrix multiplication or “matmul” takes,and the communication latency measures how long it takes to propagateresults between the GPUs.We anchor our estimates of these two latencies to commonly used machinelearning hardware. Experiments by Erdil and Schneider-Joseph (2024)indicate a kernel latency on the order of 4.5 µs for an A100 GPU. Meanwhile,the communication latency in an 8-GPU NVLINK pod for an all-reduce is onthe order of 9.2 µs.93 The total base latency per matmul in an NVLINK pod isthen in the order of 13.7 µs.The latency of each transformer layer follows straightforwardly from this. Inparticular, each layer of a standard decoder-only transformer modelinvolves four consecutive matrix multiplications,94 and we must pass eachlayer twice (for the forward and backward passes). Therefore, the minimallatency per layer and batch is eight times that of a single matrixmultiplication, i.e. .To finish estimating the largest training run allowed by the latency wall, weneed to make some assumption on scaling the number of layers and theamount of training data. As a heuristic, let’s assume that the number oflayers in a model is roughly the cube root of the number ofparameters,95 and that the training dataset size will scale proportionallywith the number of parameters, following the Chinchilla rule. In particular,assuming a 120 µs minimum latency per layer and a batch size of60M tokens, we would find that the largest model that can be trained innine months is 700T parameters, which allows for Chinchilla-optimalmodels of up to 6e31 FLOP. Note that this estimate might prove to be toooptimistic if the latencies per all-reduce from the NVIDIA CollectiveCommunications Library (NCCL) prove to be slower than reported forintermediate size messages.96,97,98 Latency wall given latencies between nodesSo far we have only accounted for intranode (within-node) latencies. Thismakes sense to some extent; tensor parallelism is often entirely conductedwithin 8-GPU NVLINK pods precisely to avoid communicating between8×13.7μs=110μs9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

35/65

nodes for each sequential matrix multiplication. However, continuedscaling would require internode communication, increasing the latency.In particular, using a standard InfiniBand tree topology, the latencybetween nodes scales logarithmically with the number of nodescommunicating. Using the NVIDIA Collective Communications Library(NCCL), an all-reduce operation will take at least , where  is the numberof GPUs within a pod participating, and  is the number of podsparticipating (this includes both the communication and kernellatencies).99 For a training run using 2D tensor parallelism, the number of podscorresponds be the number of GPUs coordinating for a 2D tensor parallelcalculation. In particular, a cluster performing TP-way 2D tensor paralleltraining requires a synchronization of TP GPUs, for which we will have 2.75GPUs on average communicating within each 8-GPU pod, and pods total.For instance, a 300M H100 cluster using 2,000-way 2D tensor parallelismwould involve then  pods per all-reduce, incurring a 7.4µs + 2 x (2.75 x 0.6 µs + log2(16) x 5 µs) = 50 µs latency, which correspondsas before to a 8 x  50 µs = 400 µs latency per layer and batch. This is thecluster size that would allow training the largest possible model in undernine months and with 60M batch size, reaching 7e30 FLOP givenprojections of hardware efficiency.100How can these latencies be reduced?Communication latency might be significantly reduced with improvementsto the cluster topology. For instance, a mesh topology could bypass thelogarithmic scaling of the internode latency, at the cost of a more complexnetworking setup within data centers (since you would need to have directconnections between all nodes). Another solution might involve larger servers with more GPUs per pod, toreduce the internode latency, or more efficient communication protocols—t=7.4μs+2×(N×0.6μs+log2(M)×5μs)NM√TP/2.75√2000/2.75=169/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

36/65

for instance, for training Llama 3.1 Meta created a fork of the NVIDIACollective Communications Library (NCCL) called NCCLX that is optimizedfor high latency setups, which they claim can shave off tens ofmicroseconds during communications.Alternatively, we might look into ways to increase the batch size or reducethe number of layers. Previous research by OpenAI relates the critical batchsize – after which you see large diminishing returns to training – to howdispersed are gradients with respect to one’s training data. Based on this,Erdil and Schneider-Joseph (2024) conjecture that the batch size might bescaled with the inverse of the reducible model loss, which perChinchilla scales as roughly the cube root of the number of modelparameters.101 If this holds, it would push back the latency wall by an orderof magnitude of scaling, see figure 7 below.Little work has been done on how the number of layers ought to be scaledand whether it could be reduced. Some experimental work indicates that itis possible to prune up to half of the intermediate layers of already trainedtransformers with a small degradation of performance. This indicates thatremoving some layers before training might be possible, though it is farfrom clear. For the time being, we ignore this possibility.After accounting for uncertainties, we conclude that scaling past 1e32FLOP will require changes to the network topology, or alternative solutionsto scale batch sizes faster or layers slower than theoretical argumentswould suggest.Projected latencies in 2030 and the largest trainingruns they would enableMedian50% CI80% CILatency per layer (μs)9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

37/65

 What constraint is the most limiting?So far we have examined four main bottlenecks to AI scaling in isolation.When considered together, they imply that training runs of up to 2e29 FLOPFigure 6: Largest training runs permitted by latency, and corresponding latencies per layer.Learn more about our assumptions.CC-BY100150200250300350400450Without batch size scalingWith batch size scalingMedian50% CI80% CILargest training run in 2030 (FLOP)102510261027102810291030103110321033Without batch size scalingWith batch size scaling9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

38/65

would be feasible by the end of the decade. This would represent a roughly10,000-fold scale-up relative to current models, and it would mean that thehistorical trend of scaling could continue uninterrupted until 2030 (seefigure 7).102The dark shaded box corresponds to an interquartile range and lightshaded region to an 80% confidence interval.Largest feasible training runs in 2030 given differentconstraintsFigure 7: Conservative estimate of the largest possible training run allowed by each of thefour constraints we consider. Also plotted: point estimate of the largest frontier runexpected by 2030, assuming a 4x/year growth rate since GPT-4’s release.Learn more about our assumptions.CC-BYTraining compute (FLOP)10291030103110321033Power constraintsChip production capacityData scarcity9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

39/65

The most binding constraints are power and chip availability—see figure7. Of these two, power may be more malleable—the energy industry is lessconcentrated and there is precedent for 100 GW expansion of the powersupply, which suppliers ought to be able to execute if planning three to fiveyears in advance. Expanding chip manufacturing faces multiple challenges:key processes like advanced packaging are mostly allocated to data centerGPUs already, and building new fabs requires large capital investments andhighly specialized labor.Data stands out as the most uncertain bottleneck, with its uncertaintyrange spanning four orders of magnitude. The utility of multimodal data foradvancing reasoning capabilities may be limited, and our estimates of boththe available stock of such data, its quality, and the efficiency of currenttokenization methods are less certain than those for text-based data.Ultimately, synthetic data might enable scaling indefinitely, but at a largecompute cost.Lastly, while the latency wall is a distant constraint, it looms on the horizonas an obstacle to be navigated. It might be pushed back by adopting morecomplex network topologies, involving larger pods or more connectionsbetween pods.Will labs attempt to scale to these newheights?We find that, based on extrapolating current trends around the key AIbottlenecks, training runs of up to 2e29 FLOP will be possible by the end ofthis decade. Achieving this scale would be on-trend: The largest trainingruns to date have been of the order of 5e25 FLOP, and six more years of thehistorical trend of 4x/year would result in models trained using roughly2e29 FLOP. The price tag of the cluster needed for such a training run willbe on the order of hundreds of billions of dollars.103 Will the AI industryactually seek to train models of this scale?9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

40/65

To date, increasing the scale of AI models has consistently led to improvedcapabilities. This has instilled a scaling-focused view of AI developmentthat has resulted in the amount of spending on training runs growing byaround 2.5x/year. Early indications suggest that this is likely to continue.Notably, it has been reported that Microsoft and OpenAI are working onplans for a data center project known as “Stargate” that could cost asmuch as $100 billion, set to launch in 2028. This suggests that major techcompanies are indeed preparing to achieve the immense scales we’reconsidering.Further evidence of AI systems’ potential for sufficiently large economicreturns could emerge from scaling beyond GPT-4 to a GPT-6 equivalentmodel, coupled with substantial algorithmic improvements and post-training improvements. This evidence might manifest as newer models likeGPT-5 generating over $20 billion in revenue within their first year ofrelease; significant advancements in AI functionality, allowing models toseamlessly integrate into existing workflows, manipulate browser windowsor virtual machines, and operate independently in the background. Weexpect that such developments could convince AI labs and their backers ofthese systems’ immense potential value.The potential payoff for AI that can automate a substantial portion ofeconomic tasks is enormous. It’s plausible that an economy would investtrillions of dollars in building up its stock of compute-related capital,including data centers, semiconductor fabrication plants, and lithographymachines. To understand the scale of this potential investment, considerthat global labor compensation is approximately $60T per year. Evenwithout factoring in accelerated economic growth from AI automation, if itbecomes feasible to develop AI capable of effectively substituting forhuman labor, investing trillions of dollars to capture even a fraction of this$60T flow would be economically justified.Standard economic models predict that if AI automation reaches a pointwhere it replaces most or all human labor, economic growth couldaccelerate by tenfold or more. Over just a few decades, this acceleratedgrowth could increase economic output by several orders ofmagnitude. Given this potential, achieving complete or near-complete9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

41/65

automation earlier could be worth a substantial portion of global output.Recognizing this immense value, investors may redirect significant portionsof their capital from traditional sectors into AI development and itsessential infrastructure (energy production and distribution, semiconductorfabrication plants, data centers). This potential for unprecedentedeconomic growth could drive trillions of dollars in investment in AIdevelopment.104Settling the question of whether companies or governments will be readyto invest upwards of tens of billions of dollars in large scale training runs isultimately outside the scope of this article. But we think it is at leastplausible, which is why we’ve undertaken this analysis.ConclusionIn this article, we estimated the maximum feasible scale of AI training runsby 2030 by analyzing the availability and potential constraints key factorsrequired for scaling up training runs. We examined four categories ofbottlenecks (power constraints, chip manufacturing capacity, data scarcity,and the latency wall) to determine at what point they might render largertraining runs infeasible. Our main result: Based on current trends, trainingruns of up to 2e29 FLOP will be feasible by the end of this decade. In otherwords, it is likely feasible, by the end of the decade, for an AI lab to train amodel that would exceed GPT-4 in scale to the same degree that GPT-4eclipses GPT-2 in training compute.One of the most likely reasons that training runs above these scales mightbe infeasible is the amount of power that can be supplied by the grid.Substantially expanding the data center power supply by 2030 may bechallenging due to grid-level constraints, carbon commitments, andpolitical factors. The second key constraint is the limited capacity to manufacture tens ofmillions of H100-equivalent chips per year. Capacity might be limited ifcapital expenditure is not substantially accelerated through the next9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

42/65

decade, even if the relevant manufacturing capacity is mostly dedicated toproducing GPUs or other AI accelerators. Overall, these constraints still permit AI labs to scale at 4x/year throughthis decade, but these present major challenges that will need to beaddressed to continue progress.If AI training runs of this scale actually occur, it would be of tremendoussignificance. AI might attract investment over hundreds of billions ofdollars, becoming the largest technological project in the history ofhumankind. Sheer scale translates into more performance and generality,suggesting that we might see advances in AI by the end of the decade asmajor as what we have experienced since the beginning of the decade.Finally, through our work we have grappled with the uncertainty we face inpredicting the trajectory of AI technologies. Despite their importance,power restrictions and chip manufacturing stand out as uncertain. We willbe investigating these in more depth in future work.We thank Anson Ho, David Owen, Konstantin Pilz, Benjamin Todd, RomeoDean, Michael Dickens, Max Negele, David Schneider-Joseph, AndyLubershane, Natalia Martemianova, Ying Yi, Luke Emberson, Peter Wildeford,Jean-Stanislas Denain, Trevor Gaunt, David Mathers, Dylan Patel, CarlShulman, Ajeya Cotra, Alexander Erben, Ryan Greenblatt, Tim Fist.AppendicesAppendix A: Summary of the extrapolative modelAppendix B: Fraction of total resources allocated to thelargest training run9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

43/65

Appendix C: Bandwidth constraintsAppendix D: Equivalence between multimodal and textdataAppendix E: Computing the largest possible training rungiven variable communication latency restrictionsAppendix F: Unprecedented economic growth could drivemassive AI investmentNotes1. For LLMs, compute scaling likely accounts for the majority of pre-training performance gains. Post-training techniques can addsubstantial but highly variable improvements, depending on thespecific method and domain. When considering both, the relativeimportance of compute scaling becomes less clear. ↩2. GPT-2 was trained on around 2e21 FLOP, and GPT-4 was trained onaround 2e25 FLOP, a 10,000x increase. ↩3. Note that the estimate for the largest training run given latencyconstraints might be too optimistic, since we suspect thecommunication latencies for the multiplication of intermediate sizematrices might be less efficient than we estimate. ↩4. In this summary and throughout this article, whenever a range is givenit is to be understood as an 80% CI. ↩9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

44/65

5. However, note that other countries might end up being preferred bydevelopers. For example, Chinese providers installed over a terawattof power in the last decade, and the UAE has a friendly regulatoryprocess for securing power. ↩6. We are assuming training runs between 2 and 9 months of duration,and that for distributed training a single company will only be able toleverage between 16% and 40% of the available supply of power for AIdata centers. ↩7. That is, the state-of-the-art among GPUs that are currently widelyused. Nvidia has already announced its next generation ofBlackwell chips. We account for future GPU generations by projectingefficiency improvements through 2030. Note that Google DeepMindmostly uses Google TPUs for training, but we use Nvidia as ourbaseline because there is more available information on NVIDIAproducts than TPUs, and because they are overwhelmingly popularamong the AI industry overall. ↩8. Based on the average ratio between the H100 DGX SuperPOD serverexpected average power (EAP) to the TDP of individual GPUs, we findan overhead factor of 1.4MW / 1016 GPUs / 700W per GPU =  1.96x (seeTable 4 here). Additionally, the power usage effectiveness (PUE),which accounts for data center-level overhead like cooling and powerdistribution, in e.g. Patterson et al (2021) is reported as a1.2x additional factor. This results in 700 * 1.96 * 1.2 = ~1700W per GPU.Here we follow our previous work (see Appendix A.4 from thepaper). ↩9. The average US household consumes 10,000 kWh of electricity a year,for an average rate of ~1,200 W after dividing by 8,760 hours in ayear. ↩10. Training compute for frontier models is growing as 4.1x/year, whichwould result in training runs of by end of thedecade. ↩3.8e25 FLOP×4.1 x/year6 years=2e29 FLOP9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

45/65

11. We derive this from an expansion of our previous research on the MLhardware power-efficiency trend. ↩12. 4-bit training has been discussed and demonstrated in particularresearch contexts, but it isn’t clear whether it will be useful in practice.We conservatively assume it won’t be. ↩13. It’s not possible to use the full capacity of 1200 GW capacity inpractice, because sources like solar, wind, and hydroelectricitygenerate power intermittently. In addition, power demand naturallyfluctuates over time. The US generated over 4,178 terawatt-hours in2023; 4178 TWh / 8760 hours in a year = 477 GW of power generationon average. ↩14. See the section “Power constraints for geographically distributedtraining” for more discussion on this figure. ↩15. One large smelter in Australia consumes 850 MW. ↩16. One interesting case study is pump storage facilities. Bath County inVirginia houses the largest pumped storage station in the US, with anpotential output capacity of 3GW. The input capacity is likely similar—”When generating power, the water flow can be as much as 13.5million US gallons (51,000 m3) per minute (850 m3/s). When storingpower, the flow can be as much as 12.7 million US gallons (48,000 m3)per minute (800 m3/s).” Another point of comparison is CERN’s LargeHadron Collider, which peaks at 200 MW consumption. ↩17. Note that since solar farms output varies with the time of the day, theaverage capacity is roughly 4x less than nominal capacity. ↩18. Notably, all of them are either hydroelectric, nuclear or based on fossilfuels. ↩19. Major electricity consumers secure power through long-termcontracts, so power from large plants will be at least partly locked bycontracts. In addition, one expert we consulted stated that the US has9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

46/65

little spare power capacity given existing supply, since many parts ofthe US are at “elevated risk” of power shortfalls. ↩20. We could find just two US plants with capacity over 3GW constructedafter the year 2000: the Manatee oil plant and the West County EnergyCenter in Florida. Most other power plants over 3GW were constructedin the 1970s. The West County Energy Center construction permit wasconceded in March 2006 and finished construction of its third reactorby July 2011, suggesting that it is feasible to build >3GW power plantsin five years. ↩21. We emphasize that this applies to power stations already existing orplanned. A determined actor might be able to build their own largescale power stations over 3 GW. ↩22. The largest utility company in Northern Virginia (Dominion) hasconnected over 4 GW of data centers since 2019, and has 5.8 GW ofcontracts with data centers. NOVEC, another Northern Virginia utility,also serves many data centers, but is a much smaller utility overall at175k customers versus Dominion’s 2.7 million. ↩23. Dominion projects an annual growth rate of 10%, which would be an~1.8x increase over six years. See the next section for more details ongrowth projections. Note that providers are likely to update their plansfor expansion as the demand for such large data centers becomesclearer—and there is precedent for aggressive expansion when facingunexpected demand. For instance, in 2022 Georgia Power said “it willneed 17 times more electricity—the equivalent of four new nuclearunits—than it had forecast just 18 months earlier because of new datacenters and manufacturing in the state.” ↩24. “The executives [at Microsoft and OpenAI] have discussed launchingStargate as soon as 2028 and expanding it through 2030, possiblyneeding as much as 5 gigawatts of power by the end”, Stargate beingthe codename for the proposed project. ↩25. We assume a 4x increase in FLOP/s per W over the H100s, and FP8performance. 100 days, which is similar to today’s training runs, would9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

47/65

result in ~7e28 FLOP. ↩26. Gemini technical report: “Training Gemini Ultra used a large fleet ofTPUv4 accelerators owned by Google across multiple data centers”. Itis not clear how many data centers were split across, or if they wereclose by. ↩27. See their ISO 27001 compliance report for the locations. ↩28. Currently, data centers in the US likely consume over 20 GW of power,on average: The IEA estimates that US data centers consumed“around 200 TWh in 2022 (~4% of US electricity demand)”, whichwould be an average rate of ~23 GW over the year. Meanwhile,McKinsey estimates US data centers consumed 17 GW on average in2022, and Goldman Sachs estimates 146 TWh in 2023, or ~17 GW onaverage. Given this range of estimates, and rapid growth in recentyears, the current rate is likely above 20 GW. ↩29. 20 / 0.6 = 33.3, and 20 / 0.4 = 50. ↩30. SemiAnalyis reports the critical IT capacity (see Figure 2) withoutaccounting for power usage effectiveness (PUE), which is total datacenter power divided by power used directly for compute rather thanoverhead functions such as cooling. Our estimate of powerconsumption per GPU also includes this overhead, so we need to usethe total capacity, not the IT capacity. We arrive at our estimates of 36GW and 48 GW by assuming a PUE of 1.5x for non-AI data centers, anda PUE of 1.2x for AI data centers, based on SemiAnalysis estimates. ↩31. This is based on backing out growth rates from two differentprojections. First, Dominion projects that data center capacity willdouble from 2023 to 2028. 2x growth over 5 years is ~15% annualgrowth. Another projection from a grid operator is that Dominion’sdata center load will grow by 4x over 15 years, and 4^(1/15) = ~1.1, or10% growth. The shorter-term projection is likely more relevant sincewe are discussing growth until 2030. ↩9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

48/65

32. From the figure above we can read 22,109 MW and 2,537 MW ofinstalled IT capacity for non-AI and AI datacenters, respectively.Assuming a PUE of 1.5x for the former and 1.2x for the latter, AIdatacenters correspond to  installedcapacity, which is equal to  of totalinstalled capacity. ↩33. Goldman Sachs projects an increase in non-AI data center energy usefrom 165 TWh in 2024 to 304 TWh in 2030, which would be a 10.7%growth rate. SemiAnalysis estimates a 8% growth rate in non-AIcritical IT capacity between end of 2023 and end of 2028. Recentannual growth in non-AI IT capacity in North America according toSemiAnalysis data was between 11% to 13%. ↩34. For comparison, both Goldman Sachs and SemiAnalysis expect aplanned expansion of AI power demand of more than 2x throughout2024. This is from a small base, so it’s unclear how long it can be keptup. ↩35. This is based on each company’s current share of the total stock of AIchips available, measured in units equivalent to the Nvidia H100 GPU.We based the share of power capacity on the share of chips becausethere was more relevant data available, and there is a strongcorrelation between power capacity and the number of chips. Our bestguess is that Google currently has the largest share, at 33% (20% to50%). See Appendix B for further details. It’s possible that somecompanies might choose to reserve a substantial fraction of theirresources for inference; here we implicitly assume they will dedicate amajority of their resources to a training cluster, which later might bereused for inference. ↩36. Gemini technical report: “Training Gemini Ultra used a large fleet ofTPUv4 accelerators owned by Google across multiple data centers”. ↩37. This corresponds to a 11,000km ring network connecting all major datacenter hubs in the US. We assume that the signal propagates at2,537MW×1.2PUE≈3GW3GW22,109MW×1.5PUE+3GW≈8%9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

49/65

200,000 km/s. This is a pessimistic assumption—distributed datacenters need not be spread this far apart. We also consider the latencyof switches for sending packets. Modern ethernet switches likeMarvell’s Teralynx 10 have latencies in the order of 500 ns per packetforward, which can be ignored in comparison to the much larger flightlatencies. ↩38. Under chinchilla-optimal scaling, a model should be trained on 20tokens per parameter. And training a model requires 6 FLOP per tokenand per parameter. So the FLOP would be 6 x 1.4e16 x 1.4e16 / 20 =5.88e31. ↩39. Other fiber optic solutions such as Infinera’s GX series might achievebetter performance, boasting 1.2 Tbps per fiber and wavelength. ↩40. The math is slightly complex, as more bandwidth results in us beingable to train larger models, but larger models take longer to performgradient updates. We explain the calculation in Appendix C. ↩41. Note that the switch-router paradigm might not be how data centerscoordinate. For instance, multiple nearby data center can beconnected via protocols like RDMA over Converged Ethernet(RoCE) to coordinate large training runs without routers mediating theconnection. Still, we consider the B4 network an illustrative example ofthe bandwidth that hypercalers can achieve between larger datacenters. ↩42. We derive this interval from a bootstrapped exponentialregression involving 33 ASIC switch controllers announced between2001 and 2023. ↩43. Increasing fiber capacity would require increasing the number of portsor the bandwidth per fiber. While long range connections up to700Tbps per fiber have been demonstrated, experts we consultedbelieve that in commercial settings we should expect bandwidths perfiber up to 3 Tbps, as techniques applied in research setting to gobeyond these connections rely on technologies like Spatial DomainMultiplexing that are believed to be commercially inferior to increasing9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

50/65

the number of fibers. The historical trend would suggest 600 Tbps /128 ports = 5 Tbps by the end of the decade, which is roughlycongruent with the expert assessment, suggesting that the trend canin fact be continued up to this point. ↩44. Two relevant cost examples for bandwidth expansion: the 200 Tbps,6,000km MAREA trans-atlantic cable cost $165 million, whilecommercial 200G last-mile underground fiber deployment ranges from$11.30 to $24.13 per foot. These costs might not be very representativeof land inter datacenter connections. ↩45. This estimate corresponds to the distributional minimum of thetraining runs allowed by distributed power and bandwidth. ↩46. This last estimate corresponds to the distribution maximum of theprevious estimate and the training runs feasible in a single data centercampus. ↩47. One natural question is whether AI developers could simply outbidexisting users to acquire more power, independent of supply growth.In the US, utilities are heavily regulated and are overseen bycommissions that scrutinize their pricing decisions. This could limithow much data centers can outbid others for power. ↩48. Full development time, including planning, permitting, andconstruction, would take longer. Solar plants require <2 yearsincluding planning time but not including permitting. ↩49. Globally, nuclear plants average 6-8 years, but the most recent nuclearplant in the US, Vogtle, took ~14 years. Major hydro plants such asBath County and Grand Coulee appear to have required roughly 10years. Wind power timelines seem quite variable and it is unclear whatthe distribution is. ↩50. Though a few recently-closed nuclear plants may be reopened withina shorter time frame. There are also companies working on varioustypes of next-generation nuclear plants, including9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

51/65

two companies backed by Sam Altman, but these seem unlikely to bedeployed at scale by 2030. ↩51. Solar power is arguably cheaper than gas overall, but requires higherupfront capital costs in exchange for very low operation costs: theEIA estimates that solar power with batteries has a capital cost of over$2k/kW; adjusting for solar’s intermittency, this would be greater than$8k/kW. Companies seeking to scale up power very aggressively mayhave high discount rates and prefer gas for that reason. ↩52. A 2020 paper estimates that carbon capture would add 3.87 cents perkWh to the cost of gas. Another estimate is that carbon capture for gascosts $79 to $88 per tonne of CO2 emissions. Given that natural gasproduces ~1 pound, or ~1/2000 tonnes of CO2 per kWh, this alsoimplies ~4 cents per kWh. ↩53. 1700 W over 270 days is ~11,000 kWh, or ~$1800 at 17 cents per kWh. ↩54. However, the total capacity of projects in the interconnection queue isenormous at 1480 GW, so it is not clear how binding this constraint is.Most projects that apply for interconnection are ultimately withdrawn,so this does not imply that 1480 GW of capacity will come online in thecoming years. ↩55. There is bipartisan interest in making it easier to approve new powerplants and energy infrastructure. ↩56. There isn’t a consensus on whether clean energy or fossil fuels aremost apt for powering AI growth. NextEra’s CEO said that wind andsolar are more promising than gas, while Ernest Moniz, former USsecretary of energy, believes that data centers will have to rely heavilyon fossil fuels. In any case, constraining the set of possible energysources presumably would make a scale-up more difficult. ↩57. Some coal plant retirements have already been delayed due to datacenter demand. ↩58. Throughout this section, we focus on Nvidia due to its dominantmarket share. However, we expect that AI chips from other firms,9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

52/65

including Google TPUs, will face similar constraints to scalingproduction. The AI chip supply chain is highly concentrated infoundries like TSMC, SK Hynix, and Samsung, so different chipmakerswill face similar constraints. ↩59. Assuming that one lab amasses ~1/4th of the 2 million H100s fortraining (see Appendix B), with 2e15 FLOP/s at FP8 precision, 40%FLOP/s utilization, and 6 months of training. ↩60. 16 million GPUs performing 2e15 FLOP/s each at 40% utilization over 6months can perform 2e29 FLOP. ↩61. This is even after assuming that performance improvements continueimproving at historical rates of around 40% per year, which would besufficient for an 8-fold increase in performance, not enough for a 20xincrease in total FLOP. ↩62. A wafer is a large piece of silicon that is used to produce many dies,which are packaged into chips. One wafer can produce enough diesfor 28 H100 chips. ↩63. It is unclear when full capacity will be reached. ↩64. This does not mean that the number of chips itself will grow at thisrate, especially if, as with the Nvidia B200, multiple dies will bepackaged into a single chip. ↩65. Sustaining the trends in AI chip performance may require a newpackaging paradigm other than CoWoS in the future, which mayrequire transitioning to new production lines. While this adds someuncertainty, our model assumes that future growth will be similar tothe recent and planned levels of growth in CoWoS. ↩66. We estimated an average of about 50,000 3nm wafers and 130,0005nm wafers per month for Q4 2023, based on corresponding revenuereports and estimated wafer prices. Similarly, China Times reportedthat 60k to 70k 3nm wpm was expected by the end of 2023.Combining the 5nm and 3nm figures, the total monthly production for5nm and 3nm process nodes at the start of 2024 was likely to be9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

53/65

around 180,000. This translates to an annualized production volume of2.2M wafers using these advanced process technologies. Note thatNVIDIA H100 GPUs are manufactured on a specialized TSMC 4N node,but this node is included under 5nm in TSMC’s revenue reports. ↩67. Assuming 28 H100 dies per wafer after accounting for defects, 2million H100 GPUs would require about 71,429 wafers. Using theestimate of 130,000 5nm wafers per month for early 2024, thisrepresents roughly 5% of TSMC’s annualized 5nm wafer capacity(71,429 / (130,000 * 12) ≈ 4.6%). ↩68. This can be approximated as , where is an additive number of percentage points. ↩69. Even this is conservative relative to Nvidia’s data centerrevenue growth over the past five years. ↩70. From 8.18M 12-inch equivalent wafers per year in 2014 to roughly 16.5Min 2023. TSMC’s planned capex for 2025 growth is in line with thesehistorical rates. ↩71. If we model production as proportional to capex raised to some power,scaling up capex growth proportionally should increase productiongrowth proportionally. So 8% / 15% * (30% to 70%) ~= 16% to 37%. ↩72. If in 2029 training and inference compute are equal, and trainingcompute continues to grow at 4x per year, one-quarter of totalcompute (equal to 2029’s training) will run inference on 2029 models,while three-quarters will train 2030 models. This suggests that 80% of2030’s compute would be used for training new models, while 20%would be used for inference on last year’s models. ↩73. Large proprietary models such as GPT-4o and Claude Opus 3.5 mighthave been trained on larger data stocks. ↩74. Following a reasoning similar to our previous work on databottlenecks, we also adjust the dataset size by 5x epochs and a 5xrevenuegrowth−margingrowth100margingrowth9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

54/65

quality penalty factor. These factors cancel out in our medianestimate. ↩75. To estimate this, we consider that the compute used to train a model isroughly six times the amount of training tokens times model size. TheChinchilla prescription of 20 tokens per parameter would recommend25 trillion parameters, and 6 * 25T * 500T = 7.5e28. ↩76. What if we are training sparse models, such as mixture-of-expertarchitectures? Sparsity allows to greatly reduce the number ofparameters involved in each forward pass. If the number of tokensused for training roughly follows Chinchilla scaling we would thenexpect data stocks to become fully utilized at lower compute scales,by at least as much as 10x given typical sparsity levels. However,scaling laws for sparse models are poorly understood—efficientlytrained models might be trained on less data. For the time being weignore this consideration. ↩77. Once AI labs run into a training data bottleneck, they can still trainlarger models without increasing the dataset size to increaseperformance, at a reduced efficiency – this is known as undertraining.For example, if the loss follows the Chinchilla parametric fit one mightbe able to undertrain models by a factor of up to 3000x with the lossgradient only reducing by a factor of 10x with respect to Chinchillascaling. Similarly, AI labs might choose to overtrain models to increasethe efficiency at inference time (as this allows to reach similarperformance with smaller models). Overtraining might cause AI labs torun into data bottlenecks sooner. For simplicity, we set asideundertraining and overtraining considerations. ↩78. Another consideration is that the stock of human-generated text datagrows over time, as population increases and as a larger share ofinternet penetration is achieved. We previously estimated that this willcontribute to grow the stock of publicly available text data by about7%/year, increasing the data stock by 50% by the end of the decade.We account for this in our final estimates, but given its smallsignificance we elide it in the discussion. ↩9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

55/65

79. We ignore here more exotic sources of data, including personalcorrespondence, recordings of user device activity and scientific andastronomical data. These might enable even further gains. ↩80. Chameleon used a mix of 37% image data, Qwen-VL used 17%, andGPT-4 allegedly used 15%. ↩81. The most expensive pure vision model in our database is ViT-22B, at4e23 FLOP, 2 orders of magnitude less than frontier language andmultimodal models. ↩82. Biological sequence databases contain billions of protein sequences,and petabytes of genetic data, but most genetic sequences are highlyredundant – only up to 0.4% of the human genome is variable acrossindividuals. The total deduplicated size is therefore likely in the tens oftrillions of tokens, significantly less than image or text data. ↩83. For instance, one could use a speech-to-text model like Whisper totranscribe the 30 million podcasts recorded last year and generate onthe order of 30M podcasts x 10 minute/podcast x 3 token/second =2.7B tokens of text. ↩84. McGrady et al (2024) report an estimate of around 14 billion videosuploaded to YouTube (updated here), with a mean duration of 615.14seconds. The total amount of video content on the internet is likely notmuch greater, as Youtube already makes up 7% of all internet traffic.(While traffic and content are not the same thing, they are likely wellcorrelated.) Lee (2021) reports that worldwide image capture is in theorder of ~1T/year, so the stock will likely be close to ~10T imagestotal. ↩85. The paper linked uses 32 tokens for the encoding, but the used tokendictionary is 70% smaller than a typical text token dictionary. ↩86. Appendix D describes a more careful estimate of the equivalencebetween modalities. In short, we make an educated guess based onexisting work on image and video tokenizers, as well as image and9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

56/65

video compression rates. We arrive at an estimate of 2 to 290 tokensper image or second of video. ↩87. As we mentioned, the internet contains on the order of ten trillionseconds of video, most of it with matching audio. However, there islikely significant redundancy between a video and its correspondingaudio. Many recordings of speech are also transcribed, as is the casewith audiobooks. A large source of mostly untranscribed audio arepodcasts: the Podcast Index has indexed 143 million podcastepisodes or about 350B seconds of audio at an average duration of~40 minutes per episode. Music is likely the least redundant type ofaudio data, and Spotify contains 100 million songs, corresponding toabout 24B seconds of music. Overall, it seems likely that there are onthe order of hundreds of billions of seconds of publicly available, non-redundant audio. ↩88. As with text, private datasets probably contain more data than publicsources. Whatsapp alone receives 7 billion voice notes per year,corresponding to 100B-1T seconds of speech, for a total of 1-10Tseconds over 10 years. ↩89. A target model trained with tenfold as much compute would have times as many parameters, and so would take FLOP to process a datapoint during training. Generating that datapoint would require  FLOP using the initial model with parameters and exploiting the inference-training tradeoff over a training gap. Therefore, the ratio between the compute used forgeneration and training on each datapoint is. ↩90. In most cases, there is a limit to how much expanding inferencecompute can benefit output quality. This plateau typically occurs afterincreasing inference compute by 1-2 orders of magnitude, though forsome specific tasks like code generation with unlimited trials, thebenefits may extend further. ↩√106×√10×N10×2NN10x10×2N FLOP6×√10×N FLOP=√103≈19/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

57/65

91. Through this section we follow the results of Erdil and Schneider-Joseph (2024). ↩92. A related bottleneck we aren’t discussing are reductions in utilizationthrough an imbalance of computation and memory access, also knownas the memory wall. This occurs with small matrices, and we in factexpect this to be more binding on current hardware than the latencybottleneck. It is unclear how the balance between memory bandwidthand hardware performance will evolve in the future. ↩93. The NCCL repository indicates a base latency of 6.8 µs, and a variableNVLINK ring latency of 0.6 µs per rank and round of communication. Ina typical NVLINK configuration one would have 8 GPUs, of which 2-3talk per matmul for 2D tensor parallelism purposes. So the latency percommunication per all-reduce is, since an all-reduce requirestwo rounds of communication. ↩94. The QKV projection, attention computation, and two consecutivelinear layers. ↩95. One natural way to scale models is to scale the model width, thefeedforward dimension and the number layers each roughlyproportionally. This is backed up in practice by some developers. Forinstance, Erdil and Schneider-Joseph (2024) calculate that Hoffmannet al (2022) scale their models as \( L = 8.8e-2 N^0.27 \), where  is thenumber of layers and  is the number of parameters. Note that thereis little existing work on “shape” scaling laws, so we are significantlyuncertain about whether this relation will hold in the future. ↩96. To step through the calculation: The training time  must be less thannine months, and is at least as high as the induced latency, where  is the minimal latency per layer and batch, Lis the number of layers and  is the number of batches processed(dataset size  divided by batch size ). Solving with the scaling assumptions \( L = 8.8e-2 N^{0.27} \) and 6.8μs+2×(3−1)×0.6μs=9.2μsLNTtL×L×D/BtLD/BDBT>tL×L×D/BD=20N9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

58/65

results in \( N < [\frac{T\times B}{20 \cdot t_L \cdot 8.8e-2}]^{1/1.27} (is the number of model parameters). This results in a maximum modelsize of 700T parameters, which following Chinchilla scaling wouldallow a training run of \( C = 6DN = 6 \cdot 20 N^2 = 6e31 \) FLOP. ↩97. We considered further optimizing the loss under a latency constraintby undertraining. This resulted in small loss reductions compared toChinchilla scaling, so we ignore this possibility. ↩98. If the models use sparsity, the compute spent per parameter of themodel might be significantly less even if the number of layers – andthus the latency – stays constant. However, it might also change theoptimal relation between the amount of data and the number of layersof the model. Scaling laws for sparse models are poorly understood, sowe ignore this possibility for the time being. ↩99. We can model the all-reduce latency involving  pods and  GPUsper pod as. Wehave values  forvalues of the base, intranode (NVLINK) and internode (InfiniBand)latencies in the low latency (LL) tree setup described in the NCCLrepository, plus a  kernel latency as in Erdil andSchneider-Joseph (2024). ↩100. Appendix E explains how we calculate this number. In essence, we setup a nonlinear system of equations where the latency is related tocluster size, and vice versa. ↩101. For Chinchilla scaling, the loss is reduced as the ~0.35th power ofmodel size. ↩102. So far the discussion has focused on 2030 as a date of interest, butthe underlying extrapolative model we have constructed can beapplied to the years up to then and afterwards. See Appendix A for adescription of the model. ↩NMNt=tKERNEL+tBASE+2×(3×tINTRA+log2(N)×tINTER)tBASE=6.8μs,tINTRA=0.6μs,tINTER=5μstKERNEL=4.5,μs9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

59/65

103. A training run of 2e29 FLOP would require 30M H100-equivalentsrunning at 40% utilization for three month. At a 44% performanceimprovement per year and a 10% increase in prices for leading MLGPUs, hardware acquisition costs would cost around $200B (orroughly half that if the training run duration were doubled). ↩104. We provide a rough calculation to illustrate this in Appendix F. ↩105. Given that generations of Nvidia data center GPUs are roughly twoyears apart, each previous generation is a factor of  lowerperformance than the next, i.e. approximately 0.48x the performance.Supposing that Microsoft bought 150,000 of the past four generationsof GPU, the total would be roughly. ↩106. Microsoft made up 19% of Nvidia’s revenue in the first quarter of 2024according to estimates from Bloomberg (via Yahoo Finance). GivenNvidia earned $22.6B in data center revenue in Q1 of this year, andassuming that Microsoft made up 19% of that, and with a price of$30,000 per H100, we land on up to 140,000 H100s for Microsoft as ofQ1 2024. ↩107. CRN reports that TechInsights estimated revenue attributable to TPUs“by looking at custom chip design revenue reported by Broadcom—which helps Google design TPUs—and how Google deploys TPUsinternally and for cloud customers. The firm then applied a fair marketprice to Google’s TPUs.” ↩About the authorsJaime Sevilla is the director of Epoch AI. His research is focused on technologicalforecasting and the trajectory of AI. He has a background in Mathematics and ComputerScience.1.442150,000×(1+0.48+0.23+0.11)=273,0009/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

60/65

Former employeeTamay Besiroglu co-founded Epoch AI and remains contributing to the organization as aresearch advisor. He left Epoch to co-lead Mechanize, a startup building virtual workenvironments, benchmarks, and training data for AI development. His research expertisefocuses on the economics of computing and broader trends in machine learning.Ben Cottier’s research interests include the diffusion of AI capabilities among actors,and measuring the effects of different inputs to AI progress. Previously, he was aResearch Fellow at Rethink Priorities, and spent time as a software engineer. Ben has abackground in machine learning.Josh You is a data analyst who collects and analyzes data on AI systems. Before EpochAI, he worked as a software engineer and a content writer, and graduated from CarletonCollege with a degree in Computer Science and Mathematics.Edu Roldán is a software developer at Epoch AI. He helps maintaining the website andassists with other programming tasks, helping the team to delve into research.Former employeePablo Villalobos has a background in Mathematics and Computer Science. Afterspending some time as a software engineer, he decided to pivot towards AI. His interestsinclude the economic consequences of advanced AI systems and the role of algorithmicimprovements in AI progress.Former employeeEge Erdil is a former researcher at Epoch AI. He has interests in mathematics, statistics,economics and forecasting.TagsTrendsComputeTraining DataHardwareRelated work9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

61/65

REPORT · 20 MIN READTraining Compute of Frontier AI Models Grows by 4-5x perYearOur expanded AI model database shows that training compute grew 4-5x/year from2010 to 2024, with similar trends in frontier and large language models.May 28, 2024 · By Jaime Sevilla and Edu Roldán9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

62/65

PAPER · 6 MIN READWill We Run Out of Data? Limits of LLM Scaling Based onHuman-Generated DataIf trends continue, language models will fully utilize the stock of human-generatedpublic text between 2026 and 2032.Jun 06, 2024 · By Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim andMarius Hobbhahn9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

63/65

Excited about our work?Talk to usSupport our researchPAPER · 4 MIN READHow Much Does It Cost to Train Frontier AI Models?The cost of training top AI models has grown 2-3x annually for the past eight years. By2027, the largest models could cost over a billion dollars.Jun 03, 2024 · Updated Jan 13, 2025 · By Ben Cottier, Robi Rahman, Loredana Fattorini, NestorMaslej and David Owen9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

64/65

Sign up for our newsletter to read the latest updates on our research and weekly commentaryon AI news and developments.Subscribe to our newsletterPUBLICATIONS & COMMENTARYPapers & ReportsNewsletterPodcastDATA & RESOURCESData on AIAI Trends & StatisticsData InsightsPROJECTSFrontierMathGATE PlaygroundDistributed TrainingModel CountsCOMPANYAbout UsOur TeamCareersConsultationsOur FundingDonateLatestContact9/29/25, 11:05 AM

Can AI Scaling Continue Through 2030? | Epoch AI

https://epoch.ai/blog/can-ai-scaling-continue-through-2030

65/65

@ 2025 Epoch AIPrivacy NoticeCookie Policy